{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "q_prefix = \"Based on the latex table and caption, \"\n",
    "\n",
    "\n",
    "def rewrite():\n",
    "    # NEW_IMG_DIR = \"new_images\"\n",
    "    # os.makedirs(NEW_IMG_DIR, exist_ok=True)\n",
    "    if os.environ.get('DATA_PATH_B'):\n",
    "        base_dir = os.environ.get('DATA_PATH_B')\n",
    "        with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "            # data_t = json.load(f)\n",
    "            data_t = list(json.load(f))[:10]\n",
    "    else:\n",
    "        base_dir = '/bohr/form-recognition-train-b6y2/v4'\n",
    "        with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "            data_t = list(json.load(f))[:10]\n",
    "    data = []\n",
    "    for d in data_t:\n",
    "        r_path = os.path.join(base_dir, \"test_images\", d[\"image_path\"])\n",
    "        # w_path = os.path.join(NEW_IMG_DIR, d[\"image_path\"])\n",
    "        question = d[\"question\"]\n",
    "        question = question[0].lower() + question[1:]\n",
    "        q3 = f\"\"\"{q_prefix}{question}\n",
    "A) {d[\"options\"][0]}\n",
    "B) {d[\"options\"][1]}\n",
    "C) {d[\"options\"][2]}\n",
    "D) {d[\"options\"][3]}\n",
    "\"\"\"\n",
    "        data.append({\n",
    "            \"r_path\": r_path,\n",
    "            # \"w_path\": w_path,\n",
    "            \"image_path\": d[\"image_path\"],\n",
    "            \"caption\": d[\"caption\"],\n",
    "            \"q3\": q3,\n",
    "        })\n",
    "\n",
    "    with open('data.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "model_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "gotp_path = \"/bohr/gotp-adm2/v1/GOT\"\n",
    "gotw_path = \"/bohr/gott-117w/v1/GOT_weights\"\n",
    "raw_cache_path = \"/bohr/cach-rxl3/v14/cache\"\n",
    "cache_path = \"./cache\"\n",
    "\n",
    "os.system(f\"cp -r {gotp_path} .\")\n",
    "os.system(f\"cp -r {raw_cache_path} .\")\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "device = \"cuda\"\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import sglang as sgl\n",
    "import torch\n",
    "from torch import multiprocessing\n",
    "from PIL import Image\n",
    "from sglang import Runtime\n",
    "from transformers import AutoTokenizer\n",
    "from GOT.model import *\n",
    "from GOT.model.plug.blip_process import BlipImageEvalProcessor\n",
    "from GOT.utils.conversation import SeparatorStyle, Conversation\n",
    "from GOT.utils.utils import KeywordsStoppingCriteria\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import logging\n",
    "# \n",
    "# multiprocessing.log_to_stderr(logging.INFO)\n",
    "# logger = multiprocessing.get_logger()\n",
    "# logging.basicConfig(filename='log.log', level=logging.INFO)\n",
    "\n",
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = '<imgpad>'\n",
    "DEFAULT_IM_START_TOKEN = '<img>'\n",
    "DEFAULT_IM_END_TOKEN = '</img>'\n",
    "\n",
    "\n",
    "class GOT:\n",
    "    def __init__(self, model_name):\n",
    "        model_name = os.path.expanduser(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = GOTQwenForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map='cuda',\n",
    "            use_safetensors=True,\n",
    "            pad_token_id=151643\n",
    "        ).eval()\n",
    "        self.model.to(device='cuda', dtype=torch.bfloat16)\n",
    "        self.image_processor = BlipImageEvalProcessor(image_size=1024)\n",
    "        self.image_processor_high = BlipImageEvalProcessor(image_size=1024)\n",
    "        image_token_len = 256\n",
    "        qs = 'OCR with format: '\n",
    "        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * image_token_len + DEFAULT_IM_END_TOKEN + '\\n' + qs\n",
    "        conv = Conversation(\n",
    "            system=\"\"\"<|im_start|>system\n",
    "        You should follow the instructions carefully and explain your answers in detail.\"\"\",\n",
    "            roles=[\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"],\n",
    "            version=\"mpt\",\n",
    "            messages=[],\n",
    "            offset=0,\n",
    "            sep_style=SeparatorStyle.MPT,\n",
    "            sep=\"<|im_end|>\",\n",
    "        )\n",
    "        conv.append_message(conv.roles[0], qs)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "\n",
    "        self.stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "        keywords = [self.stop_str]\n",
    "\n",
    "        inputs = self.tokenizer([prompt])\n",
    "        self.input_ids = torch.as_tensor(inputs.input_ids).cuda()\n",
    "        self.stopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, self.input_ids)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image_1 = image.copy()\n",
    "        image_tensor = self.image_processor(image)\n",
    "        image_tensor_1 = self.image_processor_high(image_1)\n",
    "        # streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            output_ids = self.model.generate(\n",
    "                self.input_ids,\n",
    "                images=[(image_tensor.unsqueeze(0).half().cuda(), image_tensor_1.unsqueeze(0).half().cuda())],\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                no_repeat_ngram_size=20,\n",
    "                # streamer=streamer,\n",
    "                max_new_tokens=4096,\n",
    "                stopping_criteria=[self.stopping_criteria]\n",
    "            )\n",
    "\n",
    "            outputs = self.tokenizer.decode(output_ids[0, self.input_ids.shape[1]:]).strip()\n",
    "\n",
    "            if outputs.endswith(self.stop_str):\n",
    "                outputs = outputs[:-len(self.stop_str)]\n",
    "            outputs = outputs.strip()\n",
    "        rows, cols = self.count_rows_cols(outputs)\n",
    "        return outputs, rows, cols\n",
    "\n",
    "    def count_rows_cols(self, latex_code):\n",
    "        try:\n",
    "            # 查找列数：根据表格行的定义找到表格列标识符，如 |l|c|c|c|c|\n",
    "            columns = re.search(r'\\\\begin\\{tabular\\}\\{([^\\}]+)\\}', latex_code)\n",
    "            if columns:\n",
    "                num_cols = len([c for c in columns.group(1) if c.isalpha()])\n",
    "            else:\n",
    "                num_cols = 0\n",
    "\n",
    "            # 查找行数：根据 \\hline 分隔符统计表格的行数\n",
    "            rows = latex_code.split(r'\\\\')\n",
    "            num_rows = sum(1 for row in rows if '&' in row or '\\\\rule' in row)\n",
    "\n",
    "            return num_rows, num_cols\n",
    "        except:\n",
    "            return -1, -1\n",
    "\n",
    "\n",
    "q2 = f\"\"\"{q_prefix}which subject is most relevant to the table or caption?\n",
    "A) Physics\n",
    "B) Mathematics\n",
    "C) Computer Science\n",
    "D) Quantitative Biology\n",
    "E) Quantitative Finance\n",
    "F) Statistics\n",
    "G) Electrical Engineering and Systems Science\n",
    "H) Economics\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@sgl.function\n",
    "def one_image(s, q2, q3):\n",
    "    s += sgl.system(\n",
    "        \"You are a helpful assistant. Provide only an label ([A-H] or [A-D]) of the correct answer for multiple-choice questions.\")\n",
    "    s += sgl.user(q2)\n",
    "    s += sgl.assistant(\n",
    "        \"subject label: \" + sgl.gen_string(\n",
    "            \"subject\",\n",
    "            # choices=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"],\n",
    "            max_tokens=2, temperature=0.0, top_p=1\n",
    "        )\n",
    "    )\n",
    "    s += sgl.user(q3)\n",
    "    s += sgl.assistant(\n",
    "        sgl.gen_string(\n",
    "            \"option\",\n",
    "            # choices=[\"A\", \"B\", \"C\", \"D\"],\n",
    "            max_tokens=2, temperature=0.0, top_p=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_out(o, s):\n",
    "    img_path, rows, cols = o\n",
    "    category = \"\"\n",
    "    answer = -1\n",
    "    try:\n",
    "        subject = s[\"subject\"]\n",
    "        match = re.search(r'[A-Za-z]', subject)\n",
    "        if match:\n",
    "            category = match.group(0).upper()\n",
    "            category = sub_list[l2i[category]]\n",
    "    except:\n",
    "        category = \"\"\n",
    "    try:\n",
    "        option = s[\"option\"]\n",
    "        match = re.search(r'[A-Za-z]', option)\n",
    "        if match:\n",
    "            answer = match.group(0).upper()\n",
    "            answer = l2i[answer]\n",
    "    except:\n",
    "        answer = -1\n",
    "    sub_item = {\n",
    "        \"image_path\": img_path,\n",
    "        \"category\": category,\n",
    "        \"cols\": cols,\n",
    "        \"rows\": rows,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "    return sub_item\n",
    "\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 8\n",
    "        self.ocr_data = multiprocessing.Queue()\n",
    "\n",
    "        self.runtime = Runtime(\n",
    "            model_path=model_path,\n",
    "            # model_overide_args=model_overide_args,\n",
    "            # disable_regex_jump_forward=True,\n",
    "            # # enable_mixed_chunk=True,\n",
    "            # triton_attention_reduce_in_fp32=True,\n",
    "        )\n",
    "        sgl.set_default_backend(self.runtime)\n",
    "\n",
    "    def run(self):\n",
    "        ocr_process = multiprocessing.Process(target=self.ocr)\n",
    "        ocr_process.start()\n",
    "        self.process()\n",
    "        self.runtime.shutdown()\n",
    "        # post.join()\n",
    "\n",
    "    def ocr(self):\n",
    "        engine = GOT(gotw_path)\n",
    "        outputs = []\n",
    "        inputs = []\n",
    "        with open('data.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for item in data:\n",
    "            img = Image.open(item[\"r_path\"])\n",
    "            latex, rows, cols = engine(img)\n",
    "            q2 = f\"\"\"\"{latex}\" This is the latex code of the table with caption \"{item[\"caption\"]}\". {q_prefix}which subject is most relevant to the table or caption?\n",
    "A) Physics\n",
    "B) Mathematics\n",
    "C) Computer Science\n",
    "D) Quantitative Biology\n",
    "E) Quantitative Finance\n",
    "F) Statistics\n",
    "G) Electrical Engineering and Systems Science\n",
    "H) Economics\n",
    "\"\"\"\n",
    "            outputs.append((item[\"image_path\"], rows, cols))\n",
    "            inputs.append({\"q2\": q2, \"q3\": item[\"q3\"]})\n",
    "            if len(outputs) == self.batch_size:\n",
    "                self.ocr_data.put((outputs, inputs))\n",
    "                outputs, inputs = [], []\n",
    "        if outputs:\n",
    "            self.ocr_data.put((outputs, inputs))\n",
    "        self.ocr_data.put(None)\n",
    "\n",
    "    def process(self):\n",
    "        submission = []\n",
    "        while True:\n",
    "            try:\n",
    "                item = self.ocr_data.get(timeout=300)\n",
    "                if item is None:\n",
    "                    break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            outputs, inputs = item\n",
    "            states = one_image.run_batch(inputs)\n",
    "            for o, s in zip(outputs, states):\n",
    "                sub_item = clean_out(o, s)\n",
    "                # logger.info(sub_item)\n",
    "                submission.append(sub_item)\n",
    "        if len(submission) != 5360:\n",
    "            import sys\n",
    "            sys.exit(f\"Submission length is {len(submission)}\")\n",
    "        with open('submission.json', 'w') as f:\n",
    "            json.dump(submission, f)\n",
    "\n",
    "\n",
    "rewrite()\n",
    "worker = Worker()\n",
    "worker.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
