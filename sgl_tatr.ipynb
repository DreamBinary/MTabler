{
 "cells": [
  {
   "id": "6516b14bf2dc4d7e",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "q_prefix = \"Based on the table, caption and html structure, \"\n",
    "\n",
    "\n",
    "def rewrite():\n",
    "    import os\n",
    "    import json\n",
    "    NEW_IMG_DIR = \"new_images\"\n",
    "    os.makedirs(NEW_IMG_DIR, exist_ok=True)\n",
    "    if os.environ.get('DATA_PATH_B'):\n",
    "        base_dir = os.environ.get('DATA_PATH_B')\n",
    "    else:\n",
    "        base_dir = '/bohr/form-recognition-train-b6y2/v4'\n",
    "    with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "        data_t = json.load(f)\n",
    "    data = []\n",
    "    for d in data_t:\n",
    "        r_path = os.path.join(base_dir, \"test_images\", d[\"image_path\"])\n",
    "        w_path = os.path.join(NEW_IMG_DIR, d[\"image_path\"])\n",
    "        question = d[\"question\"]\n",
    "        question = question[0].lower() + question[1:]\n",
    "        q3 = f\"\"\"{q_prefix}{question}\n",
    "A) {d[\"options\"][0]}\n",
    "B) {d[\"options\"][1]}\n",
    "C) {d[\"options\"][2]}\n",
    "D) {d[\"options\"][3]}\n",
    "\"\"\"\n",
    "        data.append({\n",
    "            \"r_path\": r_path,\n",
    "            \"w_path\": w_path,\n",
    "            \"image_path\": d[\"image_path\"],\n",
    "            \"caption\": d[\"caption\"],\n",
    "            \"q3\": q3,\n",
    "        })\n",
    "\n",
    "    with open('data.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# import logging\n",
    "# \n",
    "# multiprocessing.log_to_stderr(logging.INFO)\n",
    "# logger = multiprocessing.get_logger()\n",
    "# logging.basicConfig(filename='sgl_unitable4.log', level=logging.INFO)\n",
    "\n",
    "p = multiprocessing.Process(target=rewrite)\n",
    "p.start()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "9e4c9e8a2c56bb3f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pkgs_path = \"/bohr/pkgs-7x29/v24/sgl_tatr_pkgs\"\n",
    "model_path = \"lmms-lab/llava-onevision-qwen2-7b-si\"\n",
    "cache_path = \"/bohr/cach-rxl3/v3/cache\"\n",
    "tatr_path = \"/bohr/tatr-xdh6/v1/tatr\"\n",
    "str_model_path = '/bohr/TATR-xmup/v1/TATR/TATR-v1.1-All-msft.pth',\n",
    "str_config_path = '/bohr/TATR-xmup/v1/TATR/structure_config.json',\n",
    "os.system(f\"pip3 install {pkgs_path}/* --ignore-installed\")\n",
    "os.system(f\"cp -r {tatr_path} .\")\n",
    "# # 提交时可能不能联网，设置成离线模式防止联网失败报错\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "device = \"cuda\"\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "id": "664dfe51317d5d0f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sglang import Runtime\n",
    "from collections import defaultdict\n",
    "from sglang.lang.chat_template import get_chat_template\n",
    "from PIL import Image\n",
    "import json\n",
    "import warnings\n",
    "import sglang as sgl\n",
    "import torch\n",
    "import multiprocessing\n",
    "import re\n",
    "from tatr import TableEngine\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "id": "3b21a43f-6086-4da8-857c-5db5ce2ef37a",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "id": "9eafee8947f97850",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "q2 = f\"\"\"{q_prefix}which subject is most relevant to the table or caption?\n",
    "A) Physics\n",
    "B) Mathematics\n",
    "C) Computer Science\n",
    "D) Quantitative Biology\n",
    "E) Quantitative Finance\n",
    "F) Statistics\n",
    "G) Electrical Engineering and Systems Science\n",
    "H) Economics\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@sgl.function\n",
    "def one_image(s, path, q1, q3):\n",
    "    s += sgl.system(\n",
    "        \"You are a helpful assistant. Provide only an label ([A-H] or [A-D]) of the correct answer for multiple-choice questions.\")\n",
    "    s += sgl.user(sgl.image(path) + q1)\n",
    "    s += sgl.assistant(\"I have a general understanding of the information in this table.\")\n",
    "    s += sgl.user(q2)\n",
    "    s += sgl.assistant(\n",
    "        sgl.gen_string(\"subject\",\n",
    "                       # choices=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"],\n",
    "                       max_tokens=2, temperature=0.0, top_p=1\n",
    "                       ))\n",
    "    s += sgl.user(q3)\n",
    "    s += sgl.assistant(\n",
    "        sgl.gen_string(\"option\",\n",
    "                       # choices=[\"A\", \"B\", \"C\", \"D\"],\n",
    "                       max_tokens=2, temperature=0.0, top_p=1\n",
    "                       ))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "9e4dd940d9c81c22",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def clean_out(o, s):\n",
    "    img_path, rows, cols = o\n",
    "    category = \"\"\n",
    "    answer = -1\n",
    "    try:\n",
    "        subject = s[\"subject\"]\n",
    "        match = re.search(r'[A-Za-z]', subject)\n",
    "        if match:\n",
    "            category = match.group(0).upper()\n",
    "            category = sub_list[l2i[category]]\n",
    "    except:\n",
    "        category = \"\"\n",
    "    try:\n",
    "        option = s[\"option\"]\n",
    "        match = re.search(r'[A-Za-z]', option)\n",
    "        if match:\n",
    "            answer = match.group(0).upper()\n",
    "            answer = l2i[answer]\n",
    "    except:\n",
    "        answer = -1\n",
    "    sub_item = {\n",
    "        \"image_path\": img_path,\n",
    "        \"category\": category,\n",
    "        \"cols\": cols,\n",
    "        \"rows\": rows,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "    return sub_item"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "ce8b4d4174b19252",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "class Worker:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 8\n",
    "        self.ocr_data = multiprocessing.Queue()\n",
    "\n",
    "    def run(self):\n",
    "        ocr_process = multiprocessing.Process(target=self.ocr)\n",
    "        ocr_process.start()\n",
    "\n",
    "        model_overide_args = {\n",
    "            \"attn_implementation\": \"eager\",\n",
    "            \"multimodal\": True,\n",
    "            \"overwrite_config\": {\n",
    "                \"image_aspect_ratio\": \"anyres_max_9\"\n",
    "            },\n",
    "        }\n",
    "        runtime = Runtime(\n",
    "            model_path=model_path,\n",
    "            model_overide_args=model_overide_args,\n",
    "            # disable_regex_jump_forward=True,\n",
    "            # # enable_mixed_chunk=True,\n",
    "            # triton_attention_reduce_in_fp32=True,\n",
    "        )\n",
    "        runtime.endpoint.chat_template = get_chat_template(\"qwen\")\n",
    "        sgl.set_default_backend(runtime)\n",
    "\n",
    "        # post = multiprocessing.Process(target=self.post_process)\n",
    "        # post.start()\n",
    "\n",
    "        self.process()\n",
    "        runtime.shutdown()\n",
    "        # post.join()\n",
    "\n",
    "    def ocr(self):\n",
    "        engine = TableEngine(\n",
    "            str_device=device,\n",
    "            str_model_path=str_model_path,\n",
    "            str_config_path=str_config_path\n",
    "        )\n",
    "        outputs = []\n",
    "        inputs = []\n",
    "        with open('data.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for item in data:\n",
    "            img = Image.open(item[\"r_path\"])\n",
    "            html, rows, cols = engine(img, item[\"w_path\"], tokens=[])\n",
    "            q1 = f'This is a table image. The caption of the table is \"{item[\"caption\"]}\". The structure of the table in html format is as follows: {html}.'\n",
    "            outputs.append((item[\"image_path\"], rows, cols))\n",
    "            inputs.append({\"path\": item[\"r_path\"], \"q1\": q1, \"q3\": item[\"q3\"]})\n",
    "            if len(outputs) == self.batch_size:\n",
    "                self.ocr_data.put((outputs, inputs))\n",
    "                outputs, inputs = [], []\n",
    "        if outputs:\n",
    "            self.ocr_data.put((outputs, inputs))\n",
    "        self.ocr_data.put(None)\n",
    "\n",
    "    def process(self):\n",
    "        flag = True\n",
    "        submission = []\n",
    "        while flag:\n",
    "            try:\n",
    "                item = self.ocr_data.get(timeout=300)\n",
    "                if item is None:\n",
    "                    break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "            outputs, inputs = item\n",
    "            states = one_image.run_batch(inputs)\n",
    "            for o, s in zip(outputs, states):\n",
    "                sub_item = clean_out(o, s)\n",
    "                submission.append(sub_item)\n",
    "        if len(submission) != 5360:\n",
    "            raise Exception(f\"Submission length is {len(submission)}\")\n",
    "        with open('submission.json', 'w') as f:\n",
    "            json.dump(submission, f)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "id": "c1b896614c8ead1f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "p.join()\n",
    "worker = Worker()\n",
    "worker.run()"
   ],
   "execution_count": 6,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
