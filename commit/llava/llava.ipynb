{
 "cells": [
  {
   "id": "9e4c9e8a2c56bb3f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# config env\n",
    "pkgs_path = \"/bohr/pkgs-7x29/v5/pkgs\"\n",
    "llava_lib_path = \"/bohr/libb-bg5b/v3/llava\"\n",
    "tsr_model_path = \"microsoft/table-structure-recognition-v1.1-all\"\n",
    "model_path = \"/bohr/llov-rplv/v2/llava-onevision-qwen2-7b-ov/\"\n",
    "cache_path = \"/bohr/cach-rxl3/v1/cache\"\n",
    "\n",
    "# pkgs_path = \"/personal/pkgs\"\n",
    "# llava_lib_path = \"/personal/llava\"\n",
    "model_path = \"lmms-lab/llava-onevision-qwen2-7b-si\"\n",
    "cache_path = \"/personal/cache\"\n",
    "\n",
    "# !pip install {pkgs_path}/*\n",
    "!cp {llava_lib_path} . -r\n",
    "\n",
    "import os\n",
    "\n",
    "# # 提交时可能不能联网，设置成离线模式防止联网失败报错\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "device = \"cuda\""
   ],
   "execution_count": 1,
   "outputs": [
    {
     "id": "c861b346-92214cf90232f96b1e4811f5_115_18",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 1,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-16T06:20:44.521719Z",
      "dependencies_met": true,
      "engine": "90cf0a6c-7019-4176-9dfd-4dabae6b1c12",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "c861b346-92214cf90232f96b1e4811f5_115_18",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "c861b346-92214cf90232f96b1e4811f5",
      "date": "2024-08-16T06:20:45.067039Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "664dfe51317d5d0f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from llava.conversation import Conversation, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "from transformers import AutoImageProcessor, TableTransformerForObjectDetection\n",
    "import json\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "import torch\n",
    "\n",
    "from PIL import ImageDraw, Image\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "id": "c861b346-92214cf90232f96b1e4811f5_115_22",
     "output_type": "stream",
     "name": "stderr",
     "text": "/opt/mamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
     "data": {
      "name": "stderr",
      "text": "/opt/mamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
     },
     "meta": {},
     "parent_header": {
      "msg_id": "c861b346-92214cf90232f96b1e4811f5_115_22",
      "msg_type": "stream",
      "username": "username",
      "session": "c861b346-92214cf90232f96b1e4811f5",
      "date": "2024-08-16T06:20:46.950296Z",
      "version": "5.3"
     }
    },
    {
     "id": "c861b346-92214cf90232f96b1e4811f5_115_23",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 2,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-16T06:20:45.069813Z",
      "dependencies_met": true,
      "engine": "90cf0a6c-7019-4176-9dfd-4dabae6b1c12",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "c861b346-92214cf90232f96b1e4811f5_115_23",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "c861b346-92214cf90232f96b1e4811f5",
      "date": "2024-08-16T06:20:49.767771Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "89f8307aac1c3988",
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T11:57:07.389264Z",
     "start_time": "2024-08-07T11:57:07.273027Z"
    }
   },
   "source": [
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    \"conv_mode\": None,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 8\n",
    "})()\n",
    "torch.cuda.empty_cache()\n",
    "disable_torch_init()\n",
    "\n",
    "llava_model_args = {\n",
    "    # \"multimodal\": True,\n",
    "}\n",
    "overwrite_config = {}\n",
    "overwrite_config[\"image_aspect_ratio\"] = \"pad\"  #√\n",
    "# overwrite_config[\"image_aspect_ratio\"] = \"anyres_max_9\" √\n",
    "# overwrite_config[\"image_aspect_ratio\"] = \"crop_split\" x\n",
    "llava_model_args[\"overwrite_config\"] = overwrite_config\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    args.model_path, args.model_base, args.model_name, device_map=\"auto\",\n",
    "    attn_implementation='eager',\n",
    "    # load_8bit=True,\n",
    "    # load_4bit=False,\n",
    "    **llava_model_args\n",
    ")\n",
    "\n",
    "tsr_model = TableTransformerForObjectDetection.from_pretrained(tsr_model_path)\n",
    "tsr_img_processor = AutoImageProcessor.from_pretrained(tsr_model_path)\n",
    "\n",
    "tsr_img_processor.size = {'height': image_processor.size[0], 'width': image_processor.size[1]}\n",
    "label2id = tsr_model.config.label2id\n",
    "label_row = label2id['table row']\n",
    "label_col = label2id['table column']"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "id": "c861b346-92214cf90232f96b1e4811f5_115_27",
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-7b-si\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nYou are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.\nOverwriting config with {'image_aspect_ratio': 'pad'}\nLoading vision tower: google/siglip-so400m-patch14-384\nLoading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.92s/it]\nModel Class: LlavaQwenForCausalLM\n(384, 384)\n",
     "data": {
      "name": "stdout",
      "text": "Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-7b-si\n"
     },
     "meta": {},
     "parent_header": {
      "msg_id": "c861b346-92214cf90232f96b1e4811f5_115_27",
      "msg_type": "stream",
      "username": "username",
      "session": "c861b346-92214cf90232f96b1e4811f5",
      "date": "2024-08-16T06:20:50.083342Z",
      "version": "5.3"
     }
    },
    {
     "id": "c861b346-92214cf90232f96b1e4811f5_115_41",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 3,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-16T06:20:49.769909Z",
      "dependencies_met": true,
      "engine": "90cf0a6c-7019-4176-9dfd-4dabae6b1c12",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "c861b346-92214cf90232f96b1e4811f5_115_41",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "c861b346-92214cf90232f96b1e4811f5",
      "date": "2024-08-16T06:21:02.278823Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "54835e82f238325c",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if os.environ.get('DATA_PATH_B'):  # 提交时会选择隐藏的测试数据集路径（A+B榜），数据集的格式与A榜数据相同，但数目不同（5360张）\n",
    "    base_dir = os.environ.get('DATA_PATH_B')\n",
    "else:\n",
    "    base_dir = '/bohr/form-recognition-train-b6y2/v4'  # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug   # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "id": "c861b346-92214cf90232f96b1e4811f5_115_45",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 4,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-16T06:21:02.281651Z",
      "dependencies_met": true,
      "engine": "90cf0a6c-7019-4176-9dfd-4dabae6b1c12",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "c861b346-92214cf90232f96b1e4811f5_115_45",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "c861b346-92214cf90232f96b1e4811f5",
      "date": "2024-08-16T06:21:02.284757Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "1b46f5337bb879e2",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "    data = json.load(f)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "id": "c861b346-92214cf90232f96b1e4811f5_115_49",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 5,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-16T06:21:02.286413Z",
      "dependencies_met": true,
      "engine": "90cf0a6c-7019-4176-9dfd-4dabae6b1c12",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "c861b346-92214cf90232f96b1e4811f5_115_49",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "c861b346-92214cf90232f96b1e4811f5",
      "date": "2024-08-16T06:21:02.296016Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "3b21a43f-6086-4da8-857c-5db5ce2ef37a",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "\n",
    "\n",
    "def clean_out(image_path, out_list):\n",
    "    # matches = re.findall(r\"\\d+\", user_input)\n",
    "    # \n",
    "    # if len(matches) == 2:\n",
    "    #     rows, columns = int(matches[0]), int(matches[1])\n",
    "    #     print(f\"Rows: {rows}, Columns: {columns}\")\n",
    "    # elif len(matches) == 1:\n",
    "    #     rows = int(matches[0])\n",
    "\n",
    "    try:\n",
    "        rows, cols = tuple(out_list[0])\n",
    "    except:\n",
    "        rows, cols = -1, -1\n",
    "    sub_item = {\n",
    "        \"image_path\": image_path,\n",
    "        \"category\": sub_list[l2i[out_list[1]]],\n",
    "        \"cols\": cols,\n",
    "        \"rows\": rows,\n",
    "        \"answer\": l2i[out_list[2]],\n",
    "    }\n",
    "    return sub_item"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "id": "c861b346-92214cf90232f96b1e4811f5_115_53",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 6,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-16T06:21:02.297605Z",
      "dependencies_met": true,
      "engine": "90cf0a6c-7019-4176-9dfd-4dabae6b1c12",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "c861b346-92214cf90232f96b1e4811f5_115_53",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "c861b346-92214cf90232f96b1e4811f5",
      "date": "2024-08-16T06:21:02.301803Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "ce8b4d4174b19252",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def tsr_process(raw_image):\n",
    "    image = raw_image.copy()\n",
    "    inputs = tsr_img_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = tsr_model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]])  # (height, width) of each image in the batch\n",
    "    results = tsr_img_processor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[0]\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    rows = 0\n",
    "    cols = 0\n",
    "    for label, box in zip(results[\"labels\"], results[\"boxes\"]):\n",
    "        label, box = label.item(), box.tolist()\n",
    "        draw.rectangle(box, outline=\"red\", width=1)\n",
    "\n",
    "        if label == label_row:\n",
    "            rows += 1\n",
    "        elif label == label_col:\n",
    "            cols += 1\n",
    "    return image, rows, cols\n",
    "\n",
    "\n",
    "def one_image(img_path, caption, qs_list):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    tsr_image, rows, cols = tsr_process(image)\n",
    "    tsr_image.show()\n",
    "    # print(rows, cols)\n",
    "    image_sizes = [\n",
    "        image.size,\n",
    "        tsr_image.size\n",
    "    ]\n",
    "    images = [image, tsr_image]\n",
    "    image_tensors = process_images(images, image_processor, model.config)\n",
    "    image_tensors = [_image.to(dtype=torch.float16, device=device) for _image in image_tensors]\n",
    "\n",
    "    out_list = []\n",
    "    with torch.inference_mode():\n",
    "        conv = Conversation(\n",
    "            system=\"\"\"<|im_start|>system\n",
    "            You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"\"\",\n",
    "            roles=[\"<|im_start|>user\", \"<|im_start|>assistant\"],\n",
    "            version=\"qwen\",\n",
    "            messages=[\n",
    "                [\"<|im_start|>user\",\n",
    "                 f'{DEFAULT_IMAGE_TOKEN}\\n The first image shows a raw table. {DEFAULT_IMAGE_TOKEN}\\n The second image displays the same table with a red-bordered outline. The caption of the table is \"{caption}\".'],\n",
    "                [\"<|im_start|>assistant\", \"I have a general understanding of the information in this table.\"]\n",
    "            ],\n",
    "            offset=0,\n",
    "            sep_style=SeparatorStyle.CHATML,\n",
    "            sep=\"<|im_end|>\",\n",
    "        )\n",
    "        for qs in qs_list:\n",
    "            conv.append_message(conv.roles[0], qs)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(\n",
    "                0).cuda()\n",
    "\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                images=image_tensors,\n",
    "                image_sizes=image_sizes,\n",
    "                do_sample=True if args.temperature > 0 else False,\n",
    "                temperature=args.temperature,\n",
    "                top_p=args.top_p,\n",
    "                num_beams=args.num_beams,\n",
    "                max_new_tokens=args.max_new_tokens,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "            conv.messages[-1][-1] = outputs\n",
    "            out_list.append(outputs)\n",
    "    return out_list"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "id": "c861b346-92214cf90232f96b1e4811f5_115_57",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 7,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-16T06:21:02.303759Z",
      "dependencies_met": true,
      "engine": "90cf0a6c-7019-4176-9dfd-4dabae6b1c12",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "c861b346-92214cf90232f96b1e4811f5_115_57",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "c861b346-92214cf90232f96b1e4811f5",
      "date": "2024-08-16T06:21:02.313110Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "43afc4bd-68cb-4ed9-a0f2-dd14d74d1e26",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# submission = []\n",
    "\n",
    "# for item in data:\n",
    "#     image_path = os.path.join(base_dir, 'test_images', item[\"image_path\"])\n",
    "#     qs_list = [\n",
    "#         f'Based on the second table, what is the shape of this table? Just answer in the format (rows, columns), where both rows and columns are positive integers:'\n",
    "#         f\"\"\"Based on the provided tables and caption, select the most relevant subject from (A. Physics, B. Mathematics, C. ComputerScience, D. QuantitativeBiology, E. QuantitativeFinance, F. Statistics, G. ElectricalEngineeringandSystemsScience, H. Economics). Answer with the option's letter from the given choices directly.\"\"\",\n",
    "#         f\"\"\"Based on the provided tables and caption, for the question: \"{item[\"question\"]}\", select the most correct option from (A. {item[\"options\"][0]}, B. {item[\"options\"][1]}, C. {item[\"options\"][2]}, D. {item[\"options\"][3]}). Answer with the option's letter from the given choices directly.\"\"\"\n",
    "#     ]\n",
    "#     out_list = one_image(image_path,item[\"caption\"], qs_list)\n",
    "#     sub_item = clean_out(item[\"image_path\"], out_list)\n",
    "#     submission.append(sub_item)\n",
    "\n",
    "# with open('submission.json', 'w') as f:\n",
    "#     json.dump(submission, f)"
   ],
   "execution_count": " "
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "submission = []\n",
    "device = \"cuda\"\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import time\n",
    "\n",
    "num = 100\n",
    "idx = 1500\n",
    "start = time.perf_counter()\n",
    "for item in data[idx: idx + num]:\n",
    "    image_path = os.path.join(base_dir, 'test_images', item[\"image_path\"])\n",
    "    qs_list = [\n",
    "        f'Based on the second table, what is its shape? Answer with two positive integers for rows and columns, separated by a comma:',\n",
    "        f\"\"\"Based on the provided tables and caption, select the most relevant subject from (A. Physics, B. Mathematics, C. ComputerScience, D. QuantitativeBiology, E. QuantitativeFinance, F. Statistics, G. ElectricalEngineeringandSystemsScience, H. Economics). Answer with the option's letter from the given choices directly.\"\"\",\n",
    "        f\"\"\"Based on the provided tables and caption, for the question: \"{item[\"question\"]}\", select the most correct option from (A. {item[\"options\"][0]}, B. {item[\"options\"][1]}, C. {item[\"options\"][2]}, D. {item[\"options\"][3]}). Answer with the option's letter from the given choices directly.\"\"\"\n",
    "    ]\n",
    "    out_list = one_image(image_path, item[\"caption\"], qs_list)\n",
    "    print(out_list)\n",
    "    sub_item = clean_out(item[\"image_path\"], out_list)\n",
    "    submission.append(sub_item)\n",
    "    print(qs_list[-1])\n",
    "    print(out_list)\n",
    "    print(sub_item)\n",
    "    img = mpimg.imread(image_path)\n",
    "\n",
    "    # 显示图片\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # 不显示坐标轴\n",
    "    plt.show()\n",
    "end = time.perf_counter()\n",
    "print(\"AVG\", (end - start) / num)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3c1ce0e-8c6d-4777-b673-e7c2b599916f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
