{
 "cells": [
  {
   "id": "9e4c9e8a2c56bb3f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# config env\n",
    "# pkgs_path = \"/bohr/pkgs-7x29/v5/pkgs\"\n",
    "llava_lib_path = \"/bohr/libb-bg5b/v3/llava\"\n",
    "tsr_model_path = \"microsoft/table-structure-recognition-v1.1-all\"\n",
    "\n",
    "help_model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-si\"\n",
    "main_model_path = \"lmms-lab/llava-onevision-qwen2-7b-si\"\n",
    "cache_path = \"/bohr/cach-rxl3/v4/cache\"\n",
    "\n",
    "# pkgs_path = \"/personal/pkgs\"\n",
    "# llava_lib_path = \"/personal/llava\"\n",
    "# model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "# cache_path = \"/personal/cache\"\n",
    "\n",
    "\n",
    "# !pip install {pkgs_path}/*\n",
    "!cp {llava_lib_path} . -r\n",
    "import os\n",
    "\n",
    "# # 提交时可能不能联网，设置成离线模式防止联网失败报错\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "device = \"cuda\""
   ],
   "execution_count": 1,
   "outputs": [
    {
     "id": "7e57f5db-c10faa0cbf26452a6ba1f25e_187_18",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 1,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-20T05:43:16.902680Z",
      "dependencies_met": true,
      "engine": "42915203-dc5b-4781-964e-0bb18afbba1f",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "7e57f5db-c10faa0cbf26452a6ba1f25e_187_18",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "7e57f5db-c10faa0cbf26452a6ba1f25e",
      "date": "2024-08-20T05:43:17.550424Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "664dfe51317d5d0f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from llava.conversation import Conversation, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "import json\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "import torch\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import AutoImageProcessor, TableTransformerForObjectDetection\n",
    "\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import asyncio\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "id": "7e57f5db-c10faa0cbf26452a6ba1f25e_187_22",
     "output_type": "stream",
     "name": "stderr",
     "text": "/opt/mamba/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
     "data": {
      "name": "stderr",
      "text": "/opt/mamba/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
     },
     "meta": {},
     "parent_header": {
      "msg_id": "7e57f5db-c10faa0cbf26452a6ba1f25e_187_22",
      "msg_type": "stream",
      "username": "username",
      "session": "7e57f5db-c10faa0cbf26452a6ba1f25e",
      "date": "2024-08-20T05:43:19.402447Z",
      "version": "5.3"
     }
    },
    {
     "id": "7e57f5db-c10faa0cbf26452a6ba1f25e_187_23",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 2,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-20T05:43:17.553122Z",
      "dependencies_met": true,
      "engine": "42915203-dc5b-4781-964e-0bb18afbba1f",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "7e57f5db-c10faa0cbf26452a6ba1f25e_187_23",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "7e57f5db-c10faa0cbf26452a6ba1f25e",
      "date": "2024-08-20T05:43:21.292649Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "89f8307aac1c3988",
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T11:57:07.389264Z",
     "start_time": "2024-08-07T11:57:07.273027Z"
    }
   },
   "source": [
    "args = type('Args', (), {\n",
    "    \"conv_mode\": None,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 8\n",
    "})()\n",
    "\n",
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "disable_torch_init()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "id": "7e57f5db-c10faa0cbf26452a6ba1f25e_187_27",
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-0.5b-si\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nYou are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.\nOverwriting config with {'image_aspect_ratio': 'anyres_max_9'}\nLoading vision tower: google/siglip-so400m-patch14-384\nModel Class: LlavaQwenForCausalLM\nLoaded LLaVA model: lmms-lab/llava-onevision-qwen2-7b-si\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nYou are using a model of type llava to instantiate a model of type llava_qwen. This is not supported for all configurations of models and can yield errors.\n",
     "data": {
      "name": "stdout",
      "text": "Loaded LLaVA model: lmms-lab/llava-onevision-qwen2-0.5b-si\n"
     },
     "meta": {},
     "parent_header": {
      "msg_id": "7e57f5db-c10faa0cbf26452a6ba1f25e_187_27",
      "msg_type": "stream",
      "username": "username",
      "session": "7e57f5db-c10faa0cbf26452a6ba1f25e",
      "date": "2024-08-20T05:43:21.587684Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "54835e82f238325c",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if os.environ.get('DATA_PATH_B'):  # 提交时会选择隐藏的测试数据集路径（A+B榜），数据集的格式与A榜数据相同，但数目不同（5360张）\n",
    "    base_dir = os.environ.get('DATA_PATH_B')\n",
    "else:\n",
    "    base_dir = '/bohr/form-recognition-train-b6y2/v4'  # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug   # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug"
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "id": "ce8b4d4174b19252",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def clean_out(image_path, out_list):\n",
    "    matches = re.findall(r\"\\d+\", out_list[0])\n",
    "    if len(matches) == 2:\n",
    "        rows, cols = int(matches[0]), int(matches[1])\n",
    "    elif len(matches) == 1:\n",
    "        rows = cols = int(matches[0])\n",
    "    else:\n",
    "        rows = cols = -1\n",
    "\n",
    "    sub_item = {\n",
    "        \"image_path\": image_path,\n",
    "        \"category\": sub_list[l2i[out_list[1]]],\n",
    "        \"cols\": cols,\n",
    "        \"rows\": rows,\n",
    "        \"answer\": l2i[out_list[2]],\n",
    "    }\n",
    "    return sub_item"
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "id": "43afc4bd-68cb-4ed9-a0f2-dd14d74d1e26",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Worker:\n",
    "    def __init__(self):\n",
    "        with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tsr_result = []\n",
    "        self.help_result = []\n",
    "        self.main_input = asyncio.Queue()\n",
    "        self.help_tokenizer, self.help_model, self.help_image_processor, _ = load_pretrained_model(\n",
    "            help_model_path, None, \"llava_qwen\", device_map=\"auto\",\n",
    "            attn_implementation='sdpa',\n",
    "            # load_8bit=True,\n",
    "            # load_4bit=False,\n",
    "            **{\n",
    "                \"multimodal\": True,\n",
    "                \"overwrite_config\": {\n",
    "                    \"image_aspect_ratio\": \"anyres_max_9\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # self.tokenizer, self.model, self.image_processor, _ = load_pretrained_model(\n",
    "        #     main_model_path, None, \"llava_qwen\", device_map=\"auto\",\n",
    "        #     attn_implementation='sdpa',\n",
    "        #     # load_8bit=True,\n",
    "        #     # load_4bit=False,\n",
    "        #     **{\n",
    "        #         \"multimodal\": True,\n",
    "        #         \"overwrite_config\": {\n",
    "        #             \"image_aspect_ratio\": \"anyres_max_9\"\n",
    "        #         }\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "        self.tsr_img_processor = AutoImageProcessor.from_pretrained(tsr_model_path)\n",
    "        self.tsr_img_processor.size['shortest_edge'] = self.help_image_processor.size[0]\n",
    "        self.tsr_model = TableTransformerForObjectDetection.from_pretrained(tsr_model_path)\n",
    "        label2id = self.tsr_model.config.label2id\n",
    "        self.label_row = label2id['table row']\n",
    "        self.label_col = label2id['table column']\n",
    "\n",
    "    async def run(self):\n",
    "        tasks = [\n",
    "            asyncio.create_task(self.help_process()),\n",
    "            asyncio.create_task(self.main_process()),\n",
    "            asyncio.create_task(self.tsr_process())\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    async def tsr_process(self):\n",
    "        for item in self.data:\n",
    "            path = os.path.join(base_dir, 'test_images', item[\"image_path\"])\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            inputs = self.tsr_img_processor(images=image, return_tensors=\"pt\")\n",
    "            outputs = self.tsr_model(**inputs)\n",
    "\n",
    "            target_sizes = torch.tensor([image.size[::-1]])  # (height, width) of each image in the batch\n",
    "            results = \\\n",
    "                self.tsr_img_processor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[\n",
    "                    0]\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            rows = 0\n",
    "            cols = 0\n",
    "            for label, box in zip(results[\"labels\"], results[\"boxes\"]):\n",
    "                label, box = label.item(), box.tolist()\n",
    "                draw.rectangle(box, outline=\"red\", width=1)\n",
    "                if label == self.label_row:\n",
    "                    rows += 1\n",
    "                elif label == self.label_col:\n",
    "                    cols += 1\n",
    "            if self.help_result:\n",
    "                await self.main_input.put((self.help_result.pop(0), (image, rows, cols)))\n",
    "            else:\n",
    "                self.tsr_result.append((image, rows, cols))\n",
    "\n",
    "        if not self.help_result:\n",
    "            await self.main_input.put(None)\n",
    "\n",
    "    async def help_process(self):\n",
    "        for item in self.data:\n",
    "            path = os.path.join(base_dir, 'test_images', item[\"image_path\"])\n",
    "            caption = item[\"caption\"]\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            image_sizes = [image.size]\n",
    "            images = [image]\n",
    "            image_tensors = [\n",
    "                process_images(images, self.help_image_processor, self.help_model.config)[0].to(dtype=torch.float16,\n",
    "                                                                                                device=device)]\n",
    "            conv = Conversation(\n",
    "                system=\"\"\"<|im_start|>system\n",
    "                                You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"\"\",\n",
    "                roles=[\"<|im_start|>user\", \"<|im_start|>assistant\"],\n",
    "                version=\"qwen\",\n",
    "                messages=[\n",
    "                    [\"<|im_start|>user\",\n",
    "                     f'{DEFAULT_IMAGE_TOKEN}\\n This is a table image. The caption of the table is \"{caption}\".'],\n",
    "                    [\"<|im_start|>assistant\",\n",
    "                     \"I have a general understanding of the information in this table.\"]\n",
    "                ],\n",
    "                offset=0,\n",
    "                sep_style=SeparatorStyle.CHATML,\n",
    "                sep=\"<|im_end|>\",\n",
    "            )\n",
    "            qs_list = [\n",
    "                f'Based on the provided table, what is its shape? Answer with two positive integers for rows and columns, separated by a comma:',\n",
    "                f\"\"\"Based on the provided table and caption, select the most relevant subject from (A. Physics, B. Mathematics, C. ComputerScience, D. QuantitativeBiology, E. QuantitativeFinance, F. Statistics, G. ElectricalEngineeringandSystemsScience, H. Economics). Answer with the option's letter from the given choices directly.\"\"\",\n",
    "                f\"\"\"Based on the provided table and caption, for the question: \"{item[\"question\"]}\", select the most correct option from (A. {item[\"options\"][0]}, B. {item[\"options\"][1]}, C. {item[\"options\"][2]}, D. {item[\"options\"][3]}). Answer with the option's letter from the given choices directly.\"\"\"\n",
    "            ]\n",
    "            out_list = self.one_image(self.help_model, self.help_tokenizer, image_tensors, image_sizes, conv, qs_list)\n",
    "            print(\"HELP:\", out_list)\n",
    "            if self.tsr_result:\n",
    "                await self.main_input.put(((caption, qs_list, out_list), self.tsr_result.pop(0)))\n",
    "            else:\n",
    "                self.help_result.append((caption, qs_list, out_list))\n",
    "        if not self.tsr_result:\n",
    "            await self.main_input.put(None)\n",
    "\n",
    "    # async def main_process(self):\n",
    "    #     submission = []\n",
    "    #     while True:\n",
    "    #         item = await self.main_input.get()\n",
    "    #         if item is None:\n",
    "    #             break\n",
    "    # \n",
    "    #         (caption, qs_list, out_list), (image, rows, cols) = item\n",
    "    #         image_sizes = [image.size]\n",
    "    #         images = [image]\n",
    "    #         image_tensors = [\n",
    "    #             process_images(images, self.image_processor, self.model.config)[0].to(dtype=torch.float16,\n",
    "    #                                                                                   device=device)]\n",
    "    #         conv = Conversation(\n",
    "    #             system=\"\"\"<|im_start|>system\n",
    "    #                     You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"\"\",\n",
    "    #             roles=[\"<|im_start|>user\", \"<|im_start|>assistant\"],\n",
    "    #             version=\"qwen\",\n",
    "    #             messages=[\n",
    "    #                 [\"<|im_start|>user\",\n",
    "    #                  f'{DEFAULT_IMAGE_TOKEN}\\n This is a table image with red borders. The table shape might be ({rows}, {cols}) but could vary. The caption of the table is \"{caption}\". Besides that, for the following questions, the answer from the llava-0.5 model is {out_list}, which you can use as a reference.'],\n",
    "    #                 [\"<|im_start|>assistant\", \"I have a general understanding of the information in this table.\"]\n",
    "    #             ],\n",
    "    #             offset=0,\n",
    "    #             sep_style=SeparatorStyle.CHATML,\n",
    "    #             sep=\"<|im_end|>\",\n",
    "    #         )\n",
    "    #         out_list = self.one_image(self.model, self.tokenizer, image_tensors, image_sizes, conv, qs_list)\n",
    "    #         sub_item = clean_out(item[\"image_path\"], out_list)\n",
    "    #         print(\"MAIN:\", out_list)\n",
    "    #         submission.append(sub_item)\n",
    "    #     with open('submission.json', 'w') as f:\n",
    "    #         json.dump(submission, f)\n",
    "\n",
    "    def one_image(self, model, tokenizer, image_tensors, image_sizes, conv, qs_list):\n",
    "        out_list = []\n",
    "        with torch.inference_mode():\n",
    "            for qs in qs_list:\n",
    "                conv.append_message(conv.roles[0], qs)\n",
    "                conv.append_message(conv.roles[1], None)\n",
    "                prompt = conv.get_prompt()\n",
    "                input_ids = tokenizer_image_token(\n",
    "                    prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n",
    "\n",
    "                output_ids = model.generate(\n",
    "                    input_ids,\n",
    "                    images=image_tensors,\n",
    "                    image_sizes=image_sizes,\n",
    "                    do_sample=True if args.temperature > 0 else False,\n",
    "                    temperature=args.temperature,\n",
    "                    top_p=args.top_p,\n",
    "                    num_beams=args.num_beams,\n",
    "                    max_new_tokens=args.max_new_tokens,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "                conv.messages[-1][-1] = outputs\n",
    "                out_list.append(outputs)\n",
    "        return out_list"
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "worker = Worker()\n",
    "await worker.run()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efc4a4748f9707a5",
   "execution_count": " "
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
