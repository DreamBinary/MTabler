{
 "cells": [
  {
   "id": "9e4c9e8a2c56bb3f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# config env\n",
    "pkgs_path = \"/bohr/pkgs-7x29/v5/pkgs\"\n",
    "llava_lib_path = \"/bohr/libb-bg5b/v3/llava\"\n",
    "tsr_model_path = \"microsoft/table-structure-recognition-v1.1-all\"\n",
    "model_path = \"lmms-lab/llava-onevision-qwen2-7b-si\"\n",
    "cache_path = \"/bohr/cach-rxl3/v3/cache\"\n",
    "\n",
    "# pkgs_path = \"/personal/pkgs\"\n",
    "# llava_lib_path = \"/personal/llava\"\n",
    "# model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "# cache_path = \"/personal/cache\"\n",
    "\n",
    "!pip install {pkgs_path}/*\n",
    "!cp {llava_lib_path} . -r\n",
    "\n",
    "import os\n",
    "\n",
    "# # 提交时可能不能联网，设置成离线模式防止联网失败报错\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "device = \"cuda\""
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "id": "664dfe51317d5d0f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from llava.conversation import Conversation, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "import json\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "id": "89f8307aac1c3988",
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T11:57:07.389264Z",
     "start_time": "2024-08-07T11:57:07.273027Z"
    }
   },
   "source": [
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    \"conv_mode\": None,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 8\n",
    "})()\n",
    "torch.cuda.empty_cache()\n",
    "disable_torch_init()\n",
    "\n",
    "llava_model_args = {\n",
    "    \"multimodal\": True,\n",
    "}\n",
    "overwrite_config = {}\n",
    "# overwrite_config[\"image_aspect_ratio\"] = \"pad\"\n",
    "overwrite_config[\"image_aspect_ratio\"] = \"anyres_max_9\"\n",
    "llava_model_args[\"overwrite_config\"] = overwrite_config\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    args.model_path, args.model_base, args.model_name, device_map=\"auto\",\n",
    "    attn_implementation='eager',\n",
    "    # load_8bit=True,\n",
    "    # load_4bit=False,\n",
    "    **llava_model_args\n",
    ")\n",
    "\n",
    "# tsr_img_processor = AutoImageProcessor.from_pretrained(tsr_model_path)\n",
    "# tsr_img_processor.size['shortest_edge'] = image_processor.size[0]\n",
    "# tsr_model = TableTransformerForObjectDetection.from_pretrained(tsr_model_path)\n",
    "# label2id = tsr_model.config.label2id\n",
    "# label_row = label2id['table row']\n",
    "# label_col = label2id['table column']"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "id": "54835e82f238325c",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if os.environ.get('DATA_PATH_B'):  # 提交时会选择隐藏的测试数据集路径（A+B榜），数据集的格式与A榜数据相同，但数目不同（5360张）\n",
    "    base_dir = os.environ.get('DATA_PATH_B')\n",
    "else:\n",
    "    base_dir = '/bohr/form-recognition-train-b6y2/v4'  # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug   # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "id": "1b46f5337bb879e2",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "    data = json.load(f)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "id": "3b21a43f-6086-4da8-857c-5db5ce2ef37a",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "\n",
    "\n",
    "def clean_out(image_path, out_list):\n",
    "    matches = re.findall(r\"\\d+\", out_list[0])\n",
    "    if len(matches) == 2:\n",
    "        rows, cols = int(matches[0]), int(matches[1])\n",
    "    elif len(matches) == 1:\n",
    "        rows = cols = int(matches[0])\n",
    "    else:\n",
    "        rows = cols = -1\n",
    "\n",
    "    sub_item = {\n",
    "        \"image_path\": image_path,\n",
    "        \"category\": sub_list[l2i[out_list[1]]],\n",
    "        \"cols\": cols,\n",
    "        \"rows\": rows,\n",
    "        \"answer\": l2i[out_list[2]],\n",
    "    }\n",
    "    return sub_item"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "id": "ce8b4d4174b19252",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def one_image(img_path, caption, qs_list):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_sizes = [image.size]\n",
    "    images = [image]\n",
    "    image_tensors = [process_images(images, image_processor, model.config)[0].to(dtype=torch.float16, device=device)]\n",
    "    out_list = []\n",
    "    with torch.inference_mode():\n",
    "        conv = Conversation(\n",
    "            system=\"\"\"<|im_start|>system\n",
    "            You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"\"\",\n",
    "            roles=[\"<|im_start|>user\", \"<|im_start|>assistant\"],\n",
    "            version=\"qwen\",\n",
    "            messages=[\n",
    "                [\"<|im_start|>user\",\n",
    "                 f'{DEFAULT_IMAGE_TOKEN}\\n This is a table image. The caption of the table is \"{caption}\".'],\n",
    "                [\"<|im_start|>assistant\", \"I have a general understanding of the information in this table.\"]\n",
    "            ],\n",
    "            offset=0,\n",
    "            sep_style=SeparatorStyle.CHATML,\n",
    "            sep=\"<|im_end|>\",\n",
    "        )\n",
    "        for qs in qs_list:\n",
    "            conv.append_message(conv.roles[0], qs)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(\n",
    "                0).cuda()\n",
    "\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                images=image_tensors,\n",
    "                image_sizes=image_sizes,\n",
    "                do_sample=True if args.temperature > 0 else False,\n",
    "                temperature=args.temperature,\n",
    "                top_p=args.top_p,\n",
    "                num_beams=args.num_beams,\n",
    "                max_new_tokens=args.max_new_tokens,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "            conv.messages[-1][-1] = outputs\n",
    "            out_list.append(outputs)\n",
    "    return out_list"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "id": "43afc4bd-68cb-4ed9-a0f2-dd14d74d1e26",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "submission = []\n",
    "\n",
    "for item in data:\n",
    "    image_path = os.path.join(base_dir, 'test_images', item[\"image_path\"])\n",
    "    qs_list = [\n",
    "        f'Based on the provided table, what is its shape? Answer with two positive integers for rows and columns, separated by a comma:',\n",
    "        f\"\"\"Based on the provided table and caption, select the most relevant subject from (A. Physics, B. Mathematics, C. ComputerScience, D. QuantitativeBiology, E. QuantitativeFinance, F. Statistics, G. ElectricalEngineeringandSystemsScience, H. Economics). Answer with the option's letter from the given choices directly.\"\"\",\n",
    "        f\"\"\"Based on the provided table and caption, for the question: \"{item[\"question\"]}\", select the most correct option from (A. {item[\"options\"][0]}, B. {item[\"options\"][1]}, C. {item[\"options\"][2]}, D. {item[\"options\"][3]}). Answer with the option's letter from the given choices directly.\"\"\"\n",
    "    ]\n",
    "    out_list = one_image(image_path, item[\"caption\"], qs_list)\n",
    "    sub_item = clean_out(item[\"image_path\"], out_list)\n",
    "    submission.append(sub_item)\n",
    "\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(submission, f)"
   ],
   "execution_count": 9,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
