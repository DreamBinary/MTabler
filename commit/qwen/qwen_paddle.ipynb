{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q_prefix = \"Based on the table, caption and html structure, \"\n",
    "\n",
    "\n",
    "def rewrite():\n",
    "    import os\n",
    "    import json\n",
    "    NEW_IMG_DIR = \"new_images\"\n",
    "    os.makedirs(NEW_IMG_DIR, exist_ok=True)\n",
    "    if os.environ.get('DATA_PATH_B'):\n",
    "        base_dir = os.environ.get('DATA_PATH_B')\n",
    "        with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "            data_t = json.load(f)\n",
    "    else:\n",
    "        base_dir = '/bohr/form-recognition-train-b6y2/v4'\n",
    "        with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "            data_t = list(json.load(f))[:10]\n",
    "    data = []\n",
    "    for d in data_t:\n",
    "        r_path = os.path.join(base_dir, \"test_images\", d[\"image_path\"])\n",
    "        w_path = os.path.join(NEW_IMG_DIR, d[\"image_path\"])\n",
    "        question = d[\"question\"]\n",
    "        question = question[0].lower() + question[1:]\n",
    "        q3 = f\"\"\"{q_prefix}{question}\n",
    "A) {d[\"options\"][0]}\n",
    "B) {d[\"options\"][1]}\n",
    "C) {d[\"options\"][2]}\n",
    "D) {d[\"options\"][3]}\n",
    "\"\"\"\n",
    "        data.append({\n",
    "            \"r_path\": r_path,\n",
    "            \"w_path\": w_path,\n",
    "            \"image_path\": d[\"image_path\"],\n",
    "            \"caption\": d[\"caption\"],\n",
    "            \"q3\": q3,\n",
    "        })\n",
    "\n",
    "    with open('data.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "# import logging\n",
    "#\n",
    "# multiprocessing.log_to_stderr(logging.INFO)\n",
    "# logger = multiprocessing.get_logger()\n",
    "# logging.basicConfig(filename='sgl_unitable4.log', level=logging.INFO)\n",
    "\n",
    "p = multiprocessing.Process(target=rewrite)\n",
    "p.start()\n",
    "\n",
    "pkgs_path = \"/bohr/pkgs-7x29/v21/pkgs\"\n",
    "model_path = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "cache_path = \"/bohr/cach-rxl3/v11/cache\"\n",
    "table_model_dir = \"/bohr/ocrr-zlwd/v1/ch_ppstructure_openatom_SLANetv2_infer\"\n",
    "table_char_dict_path = \"/bohr/ocrr-zlwd/v1/table_structure_dict.txt\"\n",
    "\n",
    "import os\n",
    "\n",
    "os.system(\"pip uninstall psutil -y\")\n",
    "os.system(f\"pip3 install {pkgs_path}/* --ignore-installed\")\n",
    "# os.system(f\"cp -r {llava_lib_path} .\")\n",
    "# # 提交时可能不能联网，设置成离线模式防止联网失败报错\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "device = \"cuda\"\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from transformers import AutoProcessor\n",
    "import numpy as np\n",
    "from paddleocr.ppocr.data.imaug import transform\n",
    "from paddleocr.ppstructure.table.predict_structure import TableStructurer\n",
    "import warnings\n",
    "import torch\n",
    "import multiprocessing\n",
    "import re\n",
    "import math\n",
    "from transformers import Qwen2VLForConditionalGeneration\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "torch.cuda.empty_cache()\n",
    "IMAGE_FACTOR = 28\n",
    "MIN_PIXELS = 4 * 28 * 28\n",
    "MAX_PIXELS = 16384 * 28 * 28\n",
    "MAX_RATIO = 200\n",
    "\n",
    "\n",
    "def round_by_factor(number: int, factor: int) -> int:\n",
    "    \"\"\"Returns the closest integer to 'number' that is divisible by 'factor'.\"\"\"\n",
    "    return round(number / factor) * factor\n",
    "\n",
    "\n",
    "def ceil_by_factor(number: int, factor: int) -> int:\n",
    "    \"\"\"Returns the smallest integer greater than or equal to 'number' that is divisible by 'factor'.\"\"\"\n",
    "    return math.ceil(number / factor) * factor\n",
    "\n",
    "\n",
    "def floor_by_factor(number: int, factor: int) -> int:\n",
    "    \"\"\"Returns the largest integer less than or equal to 'number' that is divisible by 'factor'.\"\"\"\n",
    "    return math.floor(number / factor) * factor\n",
    "\n",
    "\n",
    "def smart_resize(\n",
    "        height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS\n",
    "):\n",
    "    \"\"\"\n",
    "    Rescales the image so that the following conditions are met:\n",
    "\n",
    "    1. Both dimensions (height and width) are divisible by 'factor'.\n",
    "\n",
    "    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n",
    "\n",
    "    3. The aspect ratio of the image is maintained as closely as possible.\n",
    "    \"\"\"\n",
    "    if max(height, width) / min(height, width) > MAX_RATIO:\n",
    "        raise ValueError(\n",
    "            f\"absolute aspect ratio must be smaller than {MAX_RATIO}, got {max(height, width) / min(height, width)}\"\n",
    "        )\n",
    "    h_bar = max(factor, round_by_factor(height, factor))\n",
    "    w_bar = max(factor, round_by_factor(width, factor))\n",
    "    if h_bar * w_bar > max_pixels:\n",
    "        beta = math.sqrt((height * width) / max_pixels)\n",
    "        h_bar = floor_by_factor(height / beta, factor)\n",
    "        w_bar = floor_by_factor(width / beta, factor)\n",
    "    elif h_bar * w_bar < min_pixels:\n",
    "        beta = math.sqrt(min_pixels / (height * width))\n",
    "        h_bar = ceil_by_factor(height * beta, factor)\n",
    "        w_bar = ceil_by_factor(width * beta, factor)\n",
    "    return h_bar, w_bar\n",
    "\n",
    "\n",
    "class TSR(TableStructurer):\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        data = {\"image\": img}\n",
    "        data = transform(data, self.preprocess_op)\n",
    "        img = data[0]\n",
    "        if img is None:\n",
    "            return None, 0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = img.copy()\n",
    "        if self.use_onnx:\n",
    "            input_dict = {}\n",
    "            input_dict[self.input_tensor.name] = img\n",
    "            outputs = self.predictor.run(self.output_tensors, input_dict)\n",
    "        else:\n",
    "            self.input_tensor.copy_from_cpu(img)\n",
    "            self.predictor.run()\n",
    "            outputs = []\n",
    "            for output_tensor in self.output_tensors:\n",
    "                output = output_tensor.copy_to_cpu()\n",
    "                outputs.append(output)\n",
    "\n",
    "        preds = {}\n",
    "        preds[\"structure_probs\"] = outputs[1]\n",
    "        preds[\"loc_preds\"] = outputs[0]\n",
    "\n",
    "        shape_list = np.expand_dims(data[-1], axis=0)\n",
    "        post_result = self.postprocess_op(preds, [shape_list])\n",
    "\n",
    "        structure_str_list = post_result[\"structure_batch_list\"][0]\n",
    "        bbox_list = post_result[\"bbox_batch_list\"][0]\n",
    "        structure_str_list = structure_str_list[0]\n",
    "        # structure_str_list = (\n",
    "        #         [\"<html>\", \"<body>\", \"<table>\"]\n",
    "        #         + structure_str_list\n",
    "        #         + [\"</table>\", \"</body>\", \"</html>\"]\n",
    "        # )\\\n",
    "        structure_str_list = [\"<table>\"] + structure_str_list + [\"</table>\"]\n",
    "        return structure_str_list, bbox_list\n",
    "\n",
    "\n",
    "def count_rows_and_columns(html_tags):\n",
    "    rows = 0\n",
    "    max_columns = 0\n",
    "    current_columns = 0\n",
    "    rowspan_columns = {}\n",
    "    index = 0\n",
    "    columns_cnt = defaultdict(int)\n",
    "    while index < len(html_tags):\n",
    "        tag = html_tags[index]\n",
    "\n",
    "        if tag == '<tr>':\n",
    "            rows += 1\n",
    "            current_columns = 0\n",
    "\n",
    "            # Account for any ongoing rowspans from previous rows\n",
    "            for col, span in rowspan_columns.items():\n",
    "                if span > 1:\n",
    "                    current_columns += 1\n",
    "                    rowspan_columns[col] -= 1\n",
    "\n",
    "        elif tag.startswith('<td'):\n",
    "            colspan = 1\n",
    "            rowspan = 1\n",
    "\n",
    "            # Check if 'colspan' and 'rowspan' are in the subsequent strings\n",
    "            if index + 1 < len(html_tags) and 'colspan=\"' in html_tags[index + 1]:\n",
    "                colspan = int(html_tags[index + 1].strip().split('colspan=\"')[1].split('\"')[0])\n",
    "                index += 1  # Skip the colspan string\n",
    "            if index + 1 < len(html_tags) and 'rowspan=\"' in html_tags[index + 1]:\n",
    "                rowspan = int(html_tags[index + 1].strip().split('rowspan=\"')[1].split('\"')[0])\n",
    "                index += 1  # Skip the rowspan string\n",
    "\n",
    "            # Increment columns count\n",
    "            current_columns += colspan\n",
    "\n",
    "            # Track rowspans for subsequent rows\n",
    "            if rowspan > 1:\n",
    "                for _ in range(colspan):\n",
    "                    rowspan_columns[current_columns - _] = rowspan\n",
    "\n",
    "        elif tag == '</tr>':\n",
    "            columns_cnt[current_columns] += 1\n",
    "            max_columns = max(max_columns, current_columns)\n",
    "\n",
    "        index += 1\n",
    "    columns = max(columns_cnt, key=columns_cnt.get)\n",
    "    return rows, columns\n",
    "\n",
    "\n",
    "q2 = f\"\"\"{q_prefix}which subject is most relevant to the table or caption?\n",
    "A) Physics\n",
    "B) Mathematics\n",
    "C) Computer Science\n",
    "D) Quantitative Biology\n",
    "E) Quantitative Finance\n",
    "F) Statistics\n",
    "G) Electrical Engineering and Systems Science\n",
    "H) Economics\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self):\n",
    "        self.processor = None\n",
    "        self.batch_size = 8\n",
    "        self.submission = []\n",
    "        self.model = None\n",
    "        self.ocr_data = multiprocessing.Queue()\n",
    "\n",
    "    def run(self):\n",
    "        ocr_process = multiprocessing.Process(target=self.ocr)\n",
    "        ocr_process.start()\n",
    "        self.process()\n",
    "\n",
    "    def ocr(self):\n",
    "        args = type(\"Args\", (), {\n",
    "            \"table_model_dir\": table_model_dir,\n",
    "            \"table_char_dict_path\": table_char_dict_path,\n",
    "            \"use_gpu\": False,\n",
    "            # \"gpu_id\": 0,\n",
    "            # \"gpu_mem\": 500,\n",
    "            \"use_npu\": False,\n",
    "            \"use_mlu\": False,\n",
    "            \"use_xpu\": False,\n",
    "            \"precision\": \"fp32\",\n",
    "            \"benchmark\": False,\n",
    "            \"use_tensorrt\": False,\n",
    "            \"use_onnx\": False,\n",
    "            \"table_max_len\": 1024,\n",
    "            \"enable_mkldnn\": True,\n",
    "            \"table_algorithm\": \"SLANet\",\n",
    "            \"merge_no_span_structure\": True,\n",
    "            \"cpu_threads\": 16,\n",
    "        })()\n",
    "        tsr = TSR(args)\n",
    "        with open('data.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        outputs = []\n",
    "        inputs = []\n",
    "        for item in data:\n",
    "            path = item[\"r_path\"]\n",
    "            img = cv2.imread(path)\n",
    "            structure_res = tsr(img)\n",
    "            html, bbox_list = structure_res\n",
    "            # boxes = np.array(bbox_list)\n",
    "            # for box in boxes.astype(int):\n",
    "            #     x1, y1, x2, y2 = box\n",
    "            #     cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            # output_path = os.path.join(img_output_dir, item[\"image_path\"])\n",
    "            # cv2.imwrite(output_path, img)\n",
    "            rows, cols = count_rows_and_columns(html)\n",
    "            q1 = f'This is a table image. The caption of the table is \"{item[\"caption\"]}\". The structure of the table in html format is as follows: {html}.'\n",
    "\n",
    "            outputs.append((item[\"image_path\"], rows, cols))\n",
    "            inputs.append((path, q1, item[\"q3\"]))\n",
    "\n",
    "            if len(outputs) == self.batch_size:\n",
    "                self.ocr_data.put((outputs, inputs))\n",
    "                outputs, inputs = [], []\n",
    "        if outputs:\n",
    "            self.ocr_data.put((outputs, inputs))\n",
    "        self.ocr_data.put(None)\n",
    "\n",
    "    def process(self):\n",
    "        flag = True\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_path, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path)\n",
    "        while flag:\n",
    "            item = self.ocr_data.get()\n",
    "            if item is None:\n",
    "                break\n",
    "            outputs, inputs = item\n",
    "            paths, q1s, q3s = zip(*inputs)\n",
    "            self.batch(outputs, paths, q1s, q3s)\n",
    "        if len(self.submission) != 5360:\n",
    "            raise Exception(f\"Submission length is {len(self.submission)}\")\n",
    "        with open('submission.json', 'w') as f:\n",
    "            json.dump(self.submission, f)\n",
    "\n",
    "    def batch(self, outputs, paths, q1s, q3s):\n",
    "        imgs = [self.fetch_image(path) for path in paths]\n",
    "        msgs = [\n",
    "            [\n",
    "                {\"role\": \"system\",\n",
    "                 \"content\": \"You are a helpful assistant. Provide only an label ([A-H] or [A-D]) of the correct answer for multiple-choice questions.\"},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": path},\n",
    "                    {\"type\": \"text\", \"text\": q1}\n",
    "                ]},\n",
    "                {\"role\": \"assistant\",\n",
    "                 \"content\": \"I have a general understanding of the information in this table.\"},\n",
    "                {\"role\": \"user\", \"content\": q2}\n",
    "            ] for path, q1 in zip(paths, q1s)\n",
    "        ]\n",
    "        texts = [\n",
    "            self.processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "            for msg in msgs\n",
    "        ]\n",
    "        batch_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=imgs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        results = self.vllm(batch_inputs)\n",
    "        ans = []\n",
    "        for i, text in enumerate(results):\n",
    "            category = \"C) Computer Science\"\n",
    "            try:\n",
    "                match = re.search(r'[A-Za-z]', text)\n",
    "                if match:\n",
    "                    category = match.group(0).upper()\n",
    "                    category = f\"{category}) {sub_list[l2i[category]]}\"\n",
    "            except:\n",
    "                category = \"C) Computer Science\"\n",
    "            ans.append({\"subject\": category})\n",
    "            msgs[i].append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": category,\n",
    "            })\n",
    "            msgs[i].append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": q3s[i],\n",
    "            })\n",
    "\n",
    "        texts = [\n",
    "            self.processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "            for msg in msgs\n",
    "        ]\n",
    "        batch_inputs = self.processor(\n",
    "            text=texts,\n",
    "            images=imgs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        results = self.vllm(batch_inputs)\n",
    "        for i, text in enumerate(results):\n",
    "            ans[i][\"option\"] = text\n",
    "            self.clean_out(outputs[i], ans[i])\n",
    "\n",
    "    def vllm(self, batch_inputs):\n",
    "        batch_inputs = batch_inputs.to(\"cuda\")\n",
    "        generated_ids = self.model.generate(**batch_inputs, max_new_tokens=2)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(batch_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        results = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def clean_out(self, o, s):\n",
    "        img_path, rows, cols = o\n",
    "        category = \"\"\n",
    "        answer = -1\n",
    "        try:\n",
    "            subject = s[\"subject\"]\n",
    "            match = re.search(r'[A-Za-z]', subject)\n",
    "            if match:\n",
    "                category = match.group(0).upper()\n",
    "                category = sub_list[l2i[category]]\n",
    "        except:\n",
    "            category = \"\"\n",
    "        try:\n",
    "            option = s[\"option\"]\n",
    "            match = re.search(r'[A-Za-z]', option)\n",
    "            if match:\n",
    "                answer = match.group(0).upper()\n",
    "                answer = l2i[answer]\n",
    "        except:\n",
    "            answer = -1\n",
    "        sub_item = {\n",
    "            \"image_path\": img_path,\n",
    "            \"category\": category,\n",
    "            \"cols\": cols,\n",
    "            \"rows\": rows,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "        self.submission.append(sub_item)\n",
    "\n",
    "    def fetch_image(self, img_path, size_factor: int = IMAGE_FACTOR) -> Image.Image:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = img.size\n",
    "        resized_height, resized_width = smart_resize(\n",
    "            height,\n",
    "            width,\n",
    "            factor=size_factor,\n",
    "            min_pixels=MIN_PIXELS,\n",
    "            max_pixels=MAX_PIXELS,\n",
    "        )\n",
    "        img = img.resize((resized_width, resized_height))\n",
    "        return img\n",
    "\n",
    "\n",
    "p.join()\n",
    "worker = Worker()\n",
    "worker.run()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f88206e862fcc2be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
