{
 "cells": [
  {
   "id": "9e4c9e8a2c56bb3f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# config env\n",
    "pkgs_path = \"/bohr/pkgs-7x29/v7/pkgs\"\n",
    "llava_lib_path = \"/bohr/libb-bg5b/v3/llava\"\n",
    "tsr_model_path = \"microsoft/table-structure-recognition-v1.1-all\"\n",
    "\n",
    "help_model_path = \"OpenGVLab/InternVL2-2B\"\n",
    "main_model_path = \"lmms-lab/llava-onevision-qwen2-7b-si\"\n",
    "cache_path = \"/bohr/cach-rxl3/v7/cache\"\n",
    "\n",
    "pkgs_path = \"/personal/pkgs\"\n",
    "# llava_lib_path = \"/personal/llava\"\n",
    "# model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "# cache_path = \"/personal/cache\"\n",
    "\n",
    "\n",
    "# !pip install {pkgs_path}/*\n",
    "!cp {llava_lib_path} . -r\n",
    "\n",
    "import os\n",
    "\n",
    "# # 提交时可能不能联网，设置成离线模式防止联网失败报错\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "device = \"cuda\""
   ],
   "execution_count": 1,
   "outputs": [
    {
     "id": "22d53ad0-c04669f908924aab45882469_32650_6",
     "output_type": "stream",
     "name": "stdout",
     "text": "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nProcessing /personal/pkgs/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/aiohappyeyeballs-2.4.0-py3-none-any.whl\nProcessing /personal/pkgs/aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/aiosignal-1.3.1-py3-none-any.whl\nProcessing /personal/pkgs/annotated_types-0.7.0-py3-none-any.whl\nProcessing /personal/pkgs/anyio-4.4.0-py3-none-any.whl\nProcessing /personal/pkgs/async_timeout-4.0.3-py3-none-any.whl\nProcessing /personal/pkgs/attrs-24.2.0-py3-none-any.whl\nProcessing /personal/pkgs/audioread-3.0.1-py3-none-any.whl\nProcessing /personal/pkgs/bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl\nProcessing /personal/pkgs/certifi-2024.7.4-py3-none-any.whl\nProcessing /personal/pkgs/cffi-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/click-8.1.7-py3-none-any.whl\nProcessing /personal/pkgs/cloudpickle-3.0.0-py3-none-any.whl\nProcessing /personal/pkgs/cmake-3.30.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/datasets-2.21.0-py3-none-any.whl\nProcessing /personal/pkgs/decorator-5.1.1-py3-none-any.whl\nProcessing /personal/pkgs/deepspeed-0.14.5-py3-none-any.whl\nProcessing /personal/pkgs/dill-0.3.8-py3-none-any.whl\nProcessing /personal/pkgs/diskcache-5.6.3-py3-none-any.whl\nProcessing /personal/pkgs/distro-1.9.0-py3-none-any.whl\nProcessing /personal/pkgs/einops-0.8.0-py3-none-any.whl\nProcessing /personal/pkgs/exceptiongroup-1.2.2-py3-none-any.whl\nProcessing /personal/pkgs/fastapi-0.112.1-py3-none-any.whl\nProcessing /personal/pkgs/filelock-3.15.4-py3-none-any.whl\nProcessing /personal/pkgs/frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/fsspec-2024.6.1-py3-none-any.whl\nProcessing /personal/pkgs/ftfy-6.2.3-py3-none-any.whl\nProcessing /personal/pkgs/gguf-0.9.1-py3-none-any.whl\nProcessing /personal/pkgs/h11-0.14.0-py3-none-any.whl\nProcessing /personal/pkgs/hjson-3.1.0-py3-none-any.whl\nProcessing /personal/pkgs/httpcore-1.0.5-py3-none-any.whl\nProcessing /personal/pkgs/httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/httpx-0.27.0-py3-none-any.whl\nProcessing /personal/pkgs/huggingface_hub-0.24.6-py3-none-any.whl\nProcessing /personal/pkgs/idna-3.7-py3-none-any.whl\nProcessing /personal/pkgs/importlib_metadata-8.4.0-py3-none-any.whl\nProcessing /personal/pkgs/interegular-0.3.3-py37-none-any.whl\nProcessing /personal/pkgs/jinja2-3.1.4-py3-none-any.whl\nProcessing /personal/pkgs/jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/joblib-1.4.2-py3-none-any.whl\nProcessing /personal/pkgs/jsonschema-4.23.0-py3-none-any.whl\nProcessing /personal/pkgs/jsonschema_specifications-2023.12.1-py3-none-any.whl\nProcessing /personal/pkgs/lark-1.2.2-py3-none-any.whl\nProcessing /personal/pkgs/lazy_loader-0.4-py3-none-any.whl\nProcessing /personal/pkgs/librosa-0.10.2.post1-py3-none-any.whl\nProcessing /personal/pkgs/llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/lm_format_enforcer-0.10.3-py3-none-any.whl\nProcessing /personal/pkgs/mpmath-1.3.0-py3-none-any.whl\nProcessing /personal/pkgs/msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/msgspec-0.18.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/multiprocess-0.70.16-py310-none-any.whl\nProcessing /personal/pkgs/nest_asyncio-1.6.0-py3-none-any.whl\nProcessing /personal/pkgs/networkx-3.3-py3-none-any.whl\nProcessing /personal/pkgs/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl\nProcessing /personal/pkgs/numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nProcessing /personal/pkgs/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl\nProcessing /personal/pkgs/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\nProcessing /personal/pkgs/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\nProcessing /personal/pkgs/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\nProcessing /personal/pkgs/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl\nProcessing /personal/pkgs/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl\nProcessing /personal/pkgs/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl\nProcessing /personal/pkgs/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl\nProcessing /personal/pkgs/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl\nProcessing /personal/pkgs/nvidia_ml_py-12.560.30-py3-none-any.whl\nProcessing /personal/pkgs/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl\nProcessing /personal/pkgs/nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl\nProcessing /personal/pkgs/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\nProcessing /personal/pkgs/open_clip_torch-2.26.1-py3-none-any.whl\nProcessing /personal/pkgs/openai-1.42.0-py3-none-any.whl\nProcessing /personal/pkgs/outlines-0.0.46-py3-none-any.whl\nProcessing /personal/pkgs/packaging-24.1-py3-none-any.whl\nProcessing /personal/pkgs/pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl\nProcessing /personal/pkgs/platformdirs-4.2.2-py3-none-any.whl\nProcessing /personal/pkgs/pooch-1.8.2-py3-none-any.whl\nProcessing /personal/pkgs/prometheus_client-0.20.0-py3-none-any.whl\nProcessing /personal/pkgs/prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl\nProcessing /personal/pkgs/protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl\nProcessing /personal/pkgs/psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/py_cpuinfo-9.0.0-py3-none-any.whl\nProcessing /personal/pkgs/pyairports-2.1.1-py3-none-any.whl\nProcessing /personal/pkgs/pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl\nProcessing /personal/pkgs/pycountry-24.6.1-py3-none-any.whl\nProcessing /personal/pkgs/pycparser-2.22-py3-none-any.whl\nProcessing /personal/pkgs/pydantic-2.8.2-py3-none-any.whl\nProcessing /personal/pkgs/pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\nProcessing /personal/pkgs/python_dotenv-1.0.1-py3-none-any.whl\nProcessing /personal/pkgs/pytz-2024.1-py2.py3-none-any.whl\nProcessing /personal/pkgs/pyzmq-26.1.1-cp310-cp310-manylinux_2_28_x86_64.whl\nProcessing /personal/pkgs/pyzmq-26.2.0-cp310-cp310-manylinux_2_28_x86_64.whl\nProcessing /personal/pkgs/ray-2.34.0-cp310-cp310-manylinux2014_x86_64.whl\nProcessing /personal/pkgs/referencing-0.35.1-py3-none-any.whl\nProcessing /personal/pkgs/regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/requests-2.32.3-py3-none-any.whl\nProcessing /personal/pkgs/rpds_py-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/six-1.16.0-py2.py3-none-any.whl\nProcessing /personal/pkgs/sniffio-1.3.1-py3-none-any.whl\nProcessing /personal/pkgs/soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl\nProcessing /personal/pkgs/soxr-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/starlette-0.38.2-py3-none-any.whl\nProcessing /personal/pkgs/sympy-1.13.2-py3-none-any.whl\nProcessing /personal/pkgs/threadpoolctl-3.5.0-py3-none-any.whl\nProcessing /personal/pkgs/tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/timm-1.0.8-py3-none-any.whl\nProcessing /personal/pkgs/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl\nProcessing /personal/pkgs/torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl\nProcessing /personal/pkgs/tqdm-4.66.5-py3-none-any.whl\nProcessing /personal/pkgs/transformers-4.44.1-py3-none-any.whl\nProcessing /personal/pkgs/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\nProcessing /personal/pkgs/typing_extensions-4.12.2-py3-none-any.whl\nProcessing /personal/pkgs/tzdata-2024.1-py2.py3-none-any.whl\nProcessing /personal/pkgs/urllib3-2.2.2-py3-none-any.whl\nProcessing /personal/pkgs/uvicorn-0.30.6-py3-none-any.whl\nProcessing /personal/pkgs/uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/vllm-0.5.4-cp38-abi3-manylinux1_x86_64.whl\nProcessing /personal/pkgs/vllm_flash_attn-2.6.1-cp310-cp310-manylinux1_x86_64.whl\nProcessing /personal/pkgs/watchfiles-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/wcwidth-0.2.13-py2.py3-none-any.whl\nProcessing /personal/pkgs/websockets-13.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl\nProcessing /personal/pkgs/xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /personal/pkgs/zipp-3.20.0-py3-none-any.whl\n\u001B[31mERROR: Cannot install pyzmq 26.1.1 (from /personal/pkgs/pyzmq-26.1.1-cp310-cp310-manylinux_2_28_x86_64.whl) and pyzmq 26.2.0 (from /personal/pkgs/pyzmq-26.2.0-cp310-cp310-manylinux_2_28_x86_64.whl) because these package versions have conflicting dependencies.\u001B[0m\u001B[31m\n\u001B[0m\nThe conflict is caused by:\n    The user requested pyzmq 26.1.1 (from /personal/pkgs/pyzmq-26.1.1-cp310-cp310-manylinux_2_28_x86_64.whl)\n    The user requested pyzmq 26.2.0 (from /personal/pkgs/pyzmq-26.2.0-cp310-cp310-manylinux_2_28_x86_64.whl)\n\nTo fix this you could try to:\n1. loosen the range of package versions you've specified\n2. remove package versions to allow pip attempt to solve the dependency conflict\n\n\u001B[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001B[0m\u001B[31m\n\u001B[0m",
     "data": {
      "name": "stdout",
      "text": "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\nProcessing /personal/pkgs/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n"
     },
     "meta": {},
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32650_6",
      "msg_type": "stream",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:44:53.103589Z",
      "version": "5.3"
     }
    },
    {
     "id": "22d53ad0-c04669f908924aab45882469_32650_56",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 1,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-22T12:44:52.117289Z",
      "dependencies_met": true,
      "engine": "588ee301-ed1d-4881-abfd-7277713685a2",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32650_56",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:45:06.106792Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "664dfe51317d5d0f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX\n",
    "\n",
    "from llava.conversation import Conversation, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "import json\n",
    "from llava.constants import DEFAULT_IMAGE_TOKEN\n",
    "import torch\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import AutoImageProcessor, TableTransformerForObjectDetection\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from vllm import LLM, SamplingParams, TextPrompt\n",
    "import multiprocessing\n",
    "from vllm.model_executor.guided_decoding.guided_fields import LLMGuidedOptions"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "id": "22d53ad0-c04669f908924aab45882469_32650_60",
     "output_type": "stream",
     "name": "stdout",
     "text": "Please install pyav to use video processing functions.\n2024-08-22 20:45:10,276\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
     "data": {
      "name": "stdout",
      "text": "Please install pyav to use video processing functions.\n"
     },
     "meta": {},
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32650_60",
      "msg_type": "stream",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:45:08.773937Z",
      "version": "5.3"
     }
    },
    {
     "id": "22d53ad0-c04669f908924aab45882469_32650_62",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 2,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-22T12:45:06.109663Z",
      "dependencies_met": true,
      "engine": "588ee301-ed1d-4881-abfd-7277713685a2",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32650_62",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:45:14.298236Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "89f8307aac1c3988",
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T11:57:07.389264Z",
     "start_time": "2024-08-07T11:57:07.273027Z"
    }
   },
   "source": [
    "args = type('Args', (), {\n",
    "    \"conv_mode\": None,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 8\n",
    "})()\n",
    "\n",
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "disable_torch_init()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "id": "22d53ad0-c04669f908924aab45882469_32650_66",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 3,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-22T12:45:14.300577Z",
      "dependencies_met": true,
      "engine": "588ee301-ed1d-4881-abfd-7277713685a2",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32650_66",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:45:14.304788Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "54835e82f238325c",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if os.environ.get('DATA_PATH_B'):  # 提交时会选择隐藏的测试数据集路径（A+B榜），数据集的格式与A榜数据相同，但数目不同（5360张）\n",
    "    base_dir = os.environ.get('DATA_PATH_B')\n",
    "else:\n",
    "    base_dir = '/bohr/form-recognition-train-b6y2/v4'  # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug   # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "id": "22d53ad0-c04669f908924aab45882469_32650_70",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 4,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-22T12:45:14.306594Z",
      "dependencies_met": true,
      "engine": "588ee301-ed1d-4881-abfd-7277713685a2",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32650_70",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:45:14.309708Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "ce8b4d4174b19252",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def clean_out(image_path, out_list):\n",
    "    matches = re.findall(r\"\\d+\", out_list[0])\n",
    "    if len(matches) >= 2:\n",
    "        rows, cols = int(matches[0]), int(matches[1])\n",
    "    elif len(matches) == 1:\n",
    "        rows = cols = int(matches[0])\n",
    "    else:\n",
    "        rows = cols = -1\n",
    "\n",
    "    sub_item = {\n",
    "        \"image_path\": image_path,\n",
    "        \"category\": sub_list[l2i[out_list[1][0]]],\n",
    "        \"cols\": cols,\n",
    "        \"rows\": rows,\n",
    "        \"answer\": l2i[out_list[2][0]],\n",
    "    }\n",
    "    return sub_item"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "id": "22d53ad0-c04669f908924aab45882469_32650_74",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 5,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-22T12:45:14.311962Z",
      "dependencies_met": true,
      "engine": "588ee301-ed1d-4881-abfd-7277713685a2",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32650_74",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:45:14.316023Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "43afc4bd-68cb-4ed9-a0f2-dd14d74d1e26",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Worker:\n",
    "    def __init__(self):\n",
    "        with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "            # self.data = list(json.load(f))[:2]\n",
    "        manager = multiprocessing.Manager()\n",
    "        self.tsr_result = manager.list()\n",
    "        self.help_result = manager.list()\n",
    "        self.main_input = multiprocessing.Queue()\n",
    "\n",
    "    def run(self):\n",
    "        # multiprocessing.set_start_method('spawn')\n",
    "        help_process = multiprocessing.Process(target=self.help_process)\n",
    "        tsr_process = multiprocessing.Process(target=self.tsr_process)\n",
    "        help_process.start()\n",
    "        tsr_process.start()\n",
    "        # self.main_process()\n",
    "        tsr_process.join()\n",
    "        help_process.join()\n",
    "\n",
    "    def tsr_process(self):\n",
    "        tsr_img_processor = AutoImageProcessor.from_pretrained(tsr_model_path)\n",
    "        tsr_img_processor.size = {'height': 384, 'width': 384}\n",
    "        tsr_model = TableTransformerForObjectDetection.from_pretrained(tsr_model_path)\n",
    "        label2id = tsr_model.config.label2id\n",
    "        label_row = label2id['table row']\n",
    "        label_col = label2id['table column']\n",
    "        for item in self.data:\n",
    "            path = os.path.join(base_dir, 'test_images', item[\"image_path\"])\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            inputs = tsr_img_processor(images=image, return_tensors=\"pt\")\n",
    "            outputs = tsr_model(**inputs)\n",
    "\n",
    "            target_sizes = torch.tensor([image.size[::-1]])  # (height, width) of each image in the batch\n",
    "            results = \\\n",
    "                tsr_img_processor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[0]\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            rows = 0\n",
    "            cols = 0\n",
    "            for label, box in zip(results[\"labels\"], results[\"boxes\"]):\n",
    "                label, box = label.item(), box.tolist()\n",
    "                draw.rectangle(box, outline=\"red\", width=1)\n",
    "                if label == label_row:\n",
    "                    rows += 1\n",
    "                elif label == label_col:\n",
    "                    cols += 1\n",
    "            self.tsr_result.put((image, rows, cols, item))\n",
    "            # if self.help_result:\n",
    "            #     self.main_input.put((self.help_result.pop(0), (image, rows, cols)))\n",
    "            # else:\n",
    "            #     self.tsr_result.append((image, rows, cols))\n",
    "            # print(\"TSR\", rows, cols)\n",
    "            # print(\"-->> len_help\", len(self.help_result))\n",
    "        # if not self.help_result and not self.tsr_result:\n",
    "        #     self.main_input.put(None)\n",
    "        self.tsr_result.put(None)\n",
    "\n",
    "    def help_process(self):\n",
    "        model = LLM(\n",
    "            model=help_model_path,\n",
    "            trust_remote_code=True,\n",
    "            dtype=\"float16\",\n",
    "            max_model_len=4096\n",
    "        )\n",
    "        tokenizer = model.get_tokenizer()\n",
    "        stop_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\", \"<|end|>\"]\n",
    "        stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n",
    "\n",
    "        while True:\n",
    "            tsr_item = self.tsr_result.get()\n",
    "            if tsr_item is None:\n",
    "                break\n",
    "            size = self.tsr_result.qsize()\n",
    "            tsr_items = [tsr_item] + [self.tsr_result.get() for _ in range(size)]\n",
    "            images, rows, cols, items = zip(*tsr_items)\n",
    "            size += 1\n",
    "            qs_list = [\n",
    "                [\n",
    "                    f'Based on the provided table, what is its shape? Answer with two positive integers for rows and columns, separated by a comma:',\n",
    "                    f\"\"\"Based on the provided table and caption, select the most relevant subject from (A. Physics, B. Mathematics, C. ComputerScience, D. QuantitativeBiology, E. QuantitativeFinance, F. Statistics, G. ElectricalEngineeringandSystemsScience, H. Economics). Answer with the option's letter from the given choices directly.\"\"\",\n",
    "                    f\"\"\"Based on the provided table and caption, for the question: \"{item[\"question\"]}\", select the most correct option from (A. {item[\"options\"][0]}, B. {item[\"options\"][1]}, C. {item[\"options\"][2]}, D. {item[\"options\"][3]}). Answer with the option's letter from the given choices directly.\"\"\"\n",
    "                ] for item in items\n",
    "            ]\n",
    "            messages = [\n",
    "                [\n",
    "                    {'role': 'system',\n",
    "                     'content': \"You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"},\n",
    "                    {'role': 'user',\n",
    "                     'content': f'<image>\\n This is a table image with red borders. The table shape might be ({rows[i]}, {cols[i]}) but could vary. The caption of the table is \"{items[i][\"caption\"]}\".'},\n",
    "                    {'role': 'assistant',\n",
    "                     'content': \"I have a general understanding of the information in this table.\"}\n",
    "                ] for i in range(size)\n",
    "            ]\n",
    "            guided_request = [\n",
    "                LLMGuidedOptions(guided_regex=r\"\\d,\\s*\\d\"),\n",
    "                LLMGuidedOptions(\n",
    "                    guided_choice=[\"A.Physics\", \"B.Mathematics\", \"C.ComputerScience\", \"D.QuantitativeBiology\",\n",
    "                                   \"E.QuantitativeFinance\", \"F.Statistics\",\n",
    "                                   \"G.ElectricalEngineeringandSystemsScience\", \"H.Economics\"]),\n",
    "                LLMGuidedOptions(guided_regex=\"\", guided_choice=[\"A\", \"B\", \"C\", \"D\"]),\n",
    "            ]\n",
    "            out_list = [[], [], []]\n",
    "            for q_idx in range(3):\n",
    "                prompts = []\n",
    "                for i in range(size):\n",
    "                    messages[i].append({'role': 'user', 'content': f'{qs_list[i][q_idx]}'})\n",
    "                    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "                    prompt = TextPrompt(prompt=prompt, multi_modal_data={\"image\": images[i]})\n",
    "                    prompts.append(prompt)\n",
    "                # noinspection PyTypeChecker\n",
    "                outputs = model.generate(prompts=prompts,\n",
    "                                         sampling_params=SamplingParams(\n",
    "                                             temperature=0, max_tokens=64, stop_token_ids=stop_token_ids\n",
    "                                         ),\n",
    "                                         guided_options_request=guided_request[q_idx])\n",
    "                for i, output in enumerate(outputs):\n",
    "                    text = output.outputs[0].text\n",
    "                    messages[i].append({'role': 'assistant', 'content': text})\n",
    "                    out_list[q_idx].append(text)\n",
    "            out_list = list(zip(*out_list))\n",
    "            for i in range(size):\n",
    "                self.main_input.put(\n",
    "                    ((items[i][\"image_path\"], items[i][\"caption\"], qs_list[i], out_list[i]), tsr_items[i]))\n",
    "\n",
    "    def main_process(self):\n",
    "        tokenizer, model, image_processor, _ = load_pretrained_model(\n",
    "            main_model_path, None, \"llava_qwen\", device_map=\"auto\",\n",
    "            attn_implementation='sdpa',\n",
    "            # load_8bit=True,\n",
    "            # load_4bit=False,\n",
    "            **{\n",
    "                \"multimodal\": True,\n",
    "                \"overwrite_config\": {\n",
    "                    \"image_aspect_ratio\": \"anyres_max_9\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        submission = []\n",
    "        while True:\n",
    "            item = self.main_input.get()\n",
    "            # print(\"MAIN ITEM\", item)\n",
    "            if item is None:\n",
    "                break\n",
    "            (image_path, caption, qs_list, out_list), (image, rows, cols) = item\n",
    "            image_sizes = [image.size]\n",
    "            images = [image]\n",
    "            image_tensors = [\n",
    "                process_images(images, image_processor, model.config)[0].to(dtype=torch.float16, device=device)]\n",
    "            conv = Conversation(\n",
    "                system=\"\"\"<|im_start|>system\n",
    "                        You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"\"\",\n",
    "                roles=[\"<|im_start|>user\", \"<|im_start|>assistant\"],\n",
    "                version=\"qwen\",\n",
    "                messages=[\n",
    "                    [\"<|im_start|>user\",\n",
    "                     f'{DEFAULT_IMAGE_TOKEN}\\n This is a table image with red borders. The table shape might be ({rows}, {cols}) but could vary. The caption of the table is \"{caption}\". Besides that, for the following three questions, the answer from the other model is {out_list}, which you can use as a reference.'],\n",
    "                    [\"<|im_start|>assistant\", \"I have a general understanding of the information in this table.\"]\n",
    "                ],\n",
    "                offset=0,\n",
    "                sep_style=SeparatorStyle.CHATML,\n",
    "                sep=\"<|im_end|>\",\n",
    "            )\n",
    "            out_list = self.one_image(model, tokenizer, image_tensors, image_sizes, conv, qs_list)\n",
    "            sub_item = clean_out(image_path, out_list)\n",
    "            print(\"MAIN:\", out_list)\n",
    "            submission.append(sub_item)\n",
    "        with open('submission.json', 'w') as f:\n",
    "            json.dump(submission, f)\n",
    "\n",
    "    def one_image(self, model, tokenizer, image_tensors, image_sizes, conv, qs_list):\n",
    "        out_list = []\n",
    "        with torch.inference_mode():\n",
    "            for qs in qs_list:\n",
    "                conv.append_message(conv.roles[0], qs)\n",
    "                conv.append_message(conv.roles[1], None)\n",
    "                prompt = conv.get_prompt()\n",
    "                input_ids = tokenizer_image_token(\n",
    "                    prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n",
    "\n",
    "                output_ids = model.generate(\n",
    "                    input_ids,\n",
    "                    images=image_tensors,\n",
    "                    image_sizes=image_sizes,\n",
    "                    do_sample=True if args.temperature > 0 else False,\n",
    "                    temperature=args.temperature,\n",
    "                    top_p=args.top_p,\n",
    "                    num_beams=args.num_beams,\n",
    "                    max_new_tokens=args.max_new_tokens,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "                conv.messages[-1][-1] = outputs\n",
    "                out_list.append(outputs)\n",
    "        return out_list"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "id": "22d53ad0-c04669f908924aab45882469_32650_78",
     "output_type": "execute_reply",
     "data": {
      "status": "ok",
      "execution_count": 6,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-22T12:45:14.317775Z",
      "dependencies_met": true,
      "engine": "588ee301-ed1d-4881-abfd-7277713685a2",
      "status": "ok"
     },
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32650_78",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:45:14.340751Z",
      "version": "5.3"
     }
    }
   ]
  },
  {
   "id": "efc4a4748f9707a5",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "worker = Worker()\n",
    "worker.run()"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "id": "22d53ad0-c04669f908924aab45882469_32707_82",
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING 08-22 20:45:14 config.py:1454] Casting torch.bfloat16 to torch.float16.\nINFO 08-22 20:45:14 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='OpenGVLab/InternVL2-2B', speculative_config=None, tokenizer='OpenGVLab/InternVL2-2B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=OpenGVLab/InternVL2-2B, use_v2_block_manager=False, enable_prefix_caching=False)\nWARNING 08-22 20:45:14 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\nINFO 08-22 20:45:15 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 08-22 20:45:15 selector.py:54] Using XFormers backend.\nINFO 08-22 20:45:18 model_runner.py:720] Starting to load model OpenGVLab/InternVL2-2B...\nINFO 08-22 20:45:18 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 08-22 20:45:18 selector.py:54] Using XFormers backend.\nINFO 08-22 20:45:18 weight_utils.py:225] Using model weights format ['*.safetensors', '*.bin', '*.pt']\nINFO 08-22 20:45:18 weight_utils.py:269] No model.safetensors.index.json found in remote.\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.01it/s]\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]\n\nINFO 08-22 20:45:21 model_runner.py:732] Loading model weights took 4.1259 GB\nWARNING 08-22 20:45:21 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\nINFO 08-22 20:45:25 gpu_executor.py:102] # GPU blocks: 15497, # CPU blocks: 2730\nINFO 08-22 20:45:28 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 08-22 20:45:28 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 08-22 20:45:56 model_runner.py:1225] Graph capturing finished in 28 secs.\nProcess Process-2:\nTraceback (most recent call last):\n  File \"/opt/mamba/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/mamba/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/tmp/ipykernel_32650/523467326.py\", line 84, in help_process\n    LLMGuidedOptions(guided_regex=r\"\\d,\\s*\\d\"),\nNameError: name 'LLMGuidedOptions' is not defined\nProcess Process-3:\nKeyboardInterrupt\nTraceback (most recent call last):\n  File \"/opt/mamba/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/opt/mamba/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/tmp/ipykernel_32650/523467326.py\", line 32, in tsr_process\n    outputs = tsr_model(**inputs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/transformers/models/table_transformer/modeling_table_transformer.py\", line 1381, in forward\n    outputs = self.model(\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/transformers/models/table_transformer/modeling_table_transformer.py\", line 1219, in forward\n    features, position_embeddings_list = self.backbone(pixel_values, pixel_mask)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/transformers/models/table_transformer/modeling_table_transformer.py\", line 350, in forward\n    out = self.conv_encoder(pixel_values, pixel_mask)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/transformers/models/table_transformer/modeling_table_transformer.py\", line 327, in forward\n    features = self.model(pixel_values) if self.config.use_timm_backbone else self.model(pixel_values).feature_maps\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/transformers/models/resnet/modeling_resnet.py\", line 498, in forward\n    outputs = self.encoder(embedding_output, output_hidden_states=True, return_dict=True)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/transformers/models/resnet/modeling_resnet.py\", line 250, in forward\n    hidden_state = stage_module(hidden_state)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/transformers/models/resnet/modeling_resnet.py\", line 219, in forward\n    hidden_state = layer(hidden_state)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/transformers/models/resnet/modeling_resnet.py\", line 136, in forward\n    hidden_state = self.layer(hidden_state)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/transformers/models/resnet/modeling_resnet.py\", line 70, in forward\n    hidden_state = self.convolution(input)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 458, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/opt/mamba/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 454, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\n\nKeyboardInterrupt\n\n",
     "data": {
      "name": "stdout",
      "text": "WARNING 08-22 20:45:14 config.py:1454] Casting torch.bfloat16 to torch.float16.\n"
     },
     "meta": {},
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32707_82",
      "msg_type": "stream",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:45:14.464823Z",
      "version": "5.3"
     }
    },
    {
     "id": "22d53ad0-c04669f908924aab45882469_32650_83",
     "output_type": "execute_reply",
     "data": {
      "status": "error",
      "traceback": [],
      "ename": "KeyboardInterrupt",
      "evalue": "",
      "engine_info": {
       "engine_uuid": "588ee301-ed1d-4881-abfd-7277713685a2",
       "engine_id": -1,
       "method": "execute"
      },
      "execution_count": 7,
      "user_expressions": {},
      "payload": []
     },
     "meta": {
      "started": "2024-08-22T12:45:14.342612Z",
      "dependencies_met": true,
      "engine": "588ee301-ed1d-4881-abfd-7277713685a2",
      "status": "error"
     },
     "parent_header": {
      "msg_id": "22d53ad0-c04669f908924aab45882469_32650_83",
      "msg_type": "execute_reply",
      "username": "username",
      "session": "22d53ad0-c04669f908924aab45882469",
      "date": "2024-08-22T12:46:04.466781Z",
      "version": "5.3"
     }
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
