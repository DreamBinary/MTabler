{
 "cells": [
  {
   "id": "9e4c9e8a2c56bb3f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# config env\n",
    "pkgs_path = \"/bohr/pkgs-7x29/v5/pkgs\"\n",
    "llava_lib_path = \"/bohr/libb-bg5b/v3/llava\"\n",
    "tsr_model_path = \"microsoft/table-structure-recognition-v1.1-all\"\n",
    "\n",
    "help_model_path = \"OpenGVLab/InternVL2-2B\"\n",
    "main_model_path = \"lmms-lab/llava-onevision-qwen2-7b-si\"\n",
    "cache_path = \"/bohr/cach-rxl3/v7/cache\"\n",
    "\n",
    "# pkgs_path = \"/personal/pkgs\"\n",
    "# llava_lib_path = \"/personal/llava\"\n",
    "# model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "# cache_path = \"/personal/cache\"\n",
    "\n",
    "\n",
    "!pip install {pkgs_path}/*\n",
    "!cp {llava_lib_path} . -r\n",
    "\n",
    "import os\n",
    "\n",
    "# # 提交时可能不能联网，设置成离线模式防止联网失败报错\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "device = \"cuda\""
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "id": "664dfe51317d5d0f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "from llava.conversation import Conversation, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "import json\n",
    "from llava.constants import DEFAULT_IMAGE_TOKEN\n",
    "import torch\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import AutoImageProcessor, TableTransformerForObjectDetection\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from vllm import LLM, SamplingParams, TextPrompt\n",
    "from vllm.model_executor.guided_decoding.guided_fields import LLMGuidedOptions"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "id": "89f8307aac1c3988",
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T11:57:07.389264Z",
     "start_time": "2024-08-07T11:57:07.273027Z"
    }
   },
   "source": [
    "args = type('Args', (), {\n",
    "    \"conv_mode\": None,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 8\n",
    "})()\n",
    "\n",
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "disable_torch_init()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "id": "54835e82f238325c",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if os.environ.get('DATA_PATH_B'):  # 提交时会选择隐藏的测试数据集路径（A+B榜），数据集的格式与A榜数据相同，但数目不同（5360张）\n",
    "    base_dir = os.environ.get('DATA_PATH_B')\n",
    "else:\n",
    "    base_dir = '/bohr/form-recognition-train-b6y2/v4'  # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug   # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "id": "ce8b4d4174b19252",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def clean_out(image_path, out_list):\n",
    "    matches = re.findall(r\"\\d+\", out_list[0])\n",
    "    if len(matches) >= 2:\n",
    "        rows, cols = int(matches[0]), int(matches[1])\n",
    "    elif len(matches) == 1:\n",
    "        rows = cols = int(matches[0])\n",
    "    else:\n",
    "        rows = cols = -1\n",
    "\n",
    "    sub_item = {\n",
    "        \"image_path\": image_path,\n",
    "        \"category\": sub_list[l2i[out_list[1][0]]],\n",
    "        \"cols\": cols,\n",
    "        \"rows\": rows,\n",
    "        \"answer\": l2i[out_list[2][0]],\n",
    "    }\n",
    "    return sub_item"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "id": "a0eeacc4229cbba3",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "id": "43afc4bd-68cb-4ed9-a0f2-dd14d74d1e26",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Worker:\n",
    "    def __init__(self):\n",
    "        with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.tsr_result = []\n",
    "        self.help_result = []\n",
    "        self.main_input = queue.Queue()\n",
    "\n",
    "        self.help_model = AutoModel.from_pretrained(\n",
    "            help_model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            load_in_8bit=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True).eval()\n",
    "\n",
    "        self.help_tokenizer = AutoTokenizer.from_pretrained(help_model_path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "        # self.tokenizer, self.model, self.image_processor, _ = load_pretrained_model(\n",
    "        #     main_model_path, None, \"llava_qwen\", device_map=\"auto\",\n",
    "        #     attn_implementation='sdpa',\n",
    "        #     # load_8bit=True,\n",
    "        #     # load_4bit=False,\n",
    "        #     **{\n",
    "        #         \"multimodal\": True,\n",
    "        #         \"overwrite_config\": {\n",
    "        #             \"image_aspect_ratio\": \"anyres_max_9\"\n",
    "        #         }\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "        self.tsr_img_processor = AutoImageProcessor.from_pretrained(tsr_model_path)\n",
    "        self.tsr_img_processor.size = {'height': 384, 'width': 384}\n",
    "        self.tsr_model = TableTransformerForObjectDetection.from_pretrained(tsr_model_path)\n",
    "        label2id = self.tsr_model.config.label2id\n",
    "        self.label_row = label2id['table row']\n",
    "        self.label_col = label2id['table column']\n",
    "\n",
    "        self.llm = LLM(model=main_model_path)\n",
    "\n",
    "    def run(self):\n",
    "        tasks = [\n",
    "            self.tsr_process,\n",
    "            self.help_process\n",
    "        ]\n",
    "        threads = [threading.Thread(target=task) for task in tasks]\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        self.main_process()\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    def tsr_process(self):\n",
    "        for item in self.data:\n",
    "            path = os.path.join(base_dir, 'test_images', item[\"image_path\"])\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            inputs = self.tsr_img_processor(images=image, return_tensors=\"pt\")\n",
    "            outputs = self.tsr_model(**inputs)\n",
    "\n",
    "            target_sizes = torch.tensor([image.size[::-1]])  # (height, width) of each image in the batch\n",
    "            results = \\\n",
    "                self.tsr_img_processor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[\n",
    "                    0]\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            rows = 0\n",
    "            cols = 0\n",
    "            for label, box in zip(results[\"labels\"], results[\"boxes\"]):\n",
    "                label, box = label.item(), box.tolist()\n",
    "                draw.rectangle(box, outline=\"red\", width=1)\n",
    "                if label == self.label_row:\n",
    "                    rows += 1\n",
    "                elif label == self.label_col:\n",
    "                    cols += 1\n",
    "            if self.help_result:\n",
    "                self.main_input.put((self.help_result.pop(0), (image, rows, cols)))\n",
    "            else:\n",
    "                self.tsr_result.append((image, rows, cols))\n",
    "            # print(\"TSR\", rows, cols)\n",
    "            # print(\"-->> len_help\", len(self.help_result))\n",
    "        if not self.help_result:\n",
    "            self.main_input.put(None)\n",
    "\n",
    "    def help_process(self):\n",
    "        generation_config = dict(max_new_tokens=64, do_sample=False)\n",
    "        for item in self.data:\n",
    "            path = os.path.join(base_dir, 'test_images', item[\"image_path\"])\n",
    "            caption = item[\"caption\"]\n",
    "            pixel_values = load_image(path, max_num=12).to(torch.bfloat16).cuda()\n",
    "            qs_list = [\n",
    "                f'Based on the provided table, what is its shape? Answer with two positive integers for rows and columns, separated by a comma:',\n",
    "                f\"\"\"Based on the provided table and caption, select the most relevant subject from (A. Physics, B. Mathematics, C. ComputerScience, D. QuantitativeBiology, E. QuantitativeFinance, F. Statistics, G. ElectricalEngineeringandSystemsScience, H. Economics). Answer with the option's letter from the given choices directly.\"\"\",\n",
    "                f\"\"\"Based on the provided table and caption, for the question: \"{item[\"question\"]}\", select the most correct option from (A. {item[\"options\"][0]}, B. {item[\"options\"][1]}, C. {item[\"options\"][2]}, D. {item[\"options\"][3]}). Answer with the option's letter from the given choices directly.\"\"\"\n",
    "            ]\n",
    "            history = [\n",
    "                (f'<image>\\n This is a table image. The caption of the table is \"{caption}\".',\n",
    "                 'I have a general understanding of the information in this table.')\n",
    "            ]\n",
    "            out_list = []\n",
    "            for question in qs_list:\n",
    "                response, history = self.help_model.chat(self.help_tokenizer, pixel_values, question, generation_config,\n",
    "                                                         history=history, return_history=True)\n",
    "                out_list.append(response)\n",
    "\n",
    "            # print(\"HELP:\", out_list)\n",
    "            # print(\"-->> len_tsr:\", len(self.tsr_result))\n",
    "            if self.tsr_result:\n",
    "                self.main_input.put(((item[\"image_path\"], caption, qs_list, out_list), self.tsr_result.pop(0)))\n",
    "            else:\n",
    "                self.help_result.append((item[\"image_path\"], caption, qs_list, out_list))\n",
    "        if not self.tsr_result:\n",
    "            self.main_input.put(None)\n",
    "\n",
    "    def main_process(self):\n",
    "        submission = []\n",
    "        while True:\n",
    "            item = self.main_input.get()\n",
    "            if item is None:\n",
    "                break\n",
    "            size = self.main_input.qsize()\n",
    "            items = [item] + [self.main_input.get() for _ in range(size)]\n",
    "            img_paths, captions, qs_lists, out_lists, images, rows, cols = zip(*[\n",
    "                (image_path, caption, qs_list, out_list, image, row, col)\n",
    "                for (image_path, caption, qs_list, out_list), (image, row, col) in items\n",
    "            ])\n",
    "            size += 1\n",
    "            convs = [\n",
    "                Conversation(\n",
    "                    system=\"\"\"<|im_start|>system\n",
    "                        You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"\"\",\n",
    "                    roles=[\"<|im_start|>user\", \"<|im_start|>assistant\"],\n",
    "                    version=\"qwen\",\n",
    "                    messages=[\n",
    "                        [\"<|im_start|>user\",\n",
    "                         f'{DEFAULT_IMAGE_TOKEN}\\n This is a table image with red borders. The table shape might be ({r}, {c}) but could vary. The caption of the table is \"{caption}\". Besides that, for the following three questions, the answer from the other model is {out_list}, which you can use as a reference.'],\n",
    "                        [\"<|im_start|>assistant\", \"I have a general understanding of the information in this table.\"]\n",
    "                    ],\n",
    "                    offset=0,\n",
    "                    sep_style=SeparatorStyle.CHATML,\n",
    "                    sep=\"<|im_end|>\",\n",
    "                ) for r, c, caption, out_list in zip(rows, cols, captions, out_lists)\n",
    "            ]\n",
    "\n",
    "            out_lists = self.vllm_images(images, convs, qs_lists)\n",
    "            sub_items = [clean_out(img_path, out_list) for img_path, out_list in zip(img_paths, out_lists)]\n",
    "            # print(\"MAIN:\", out_list)\n",
    "            submission.extend(sub_items)\n",
    "        with open('submission.json', 'w') as f:\n",
    "            json.dump(submission, f)\n",
    "\n",
    "    def vllm_images(self, image_list, conv_list, qs_list):\n",
    "        num = len(image_list)\n",
    "        ans_list = [[], [], []]\n",
    "        sampling_params = SamplingParams(temperature=0.2, max_tokens=64, stop_token_ids=None),\n",
    "        guided_request = [\n",
    "            LLMGuidedOptions(guided_regex=\"^\\d,\\s*\\d$\"),\n",
    "            LLMGuidedOptions(guided_choice=[\"A.Physics\", \"B.Mathematics\", \"C.ComputerScience\", \"D.QuantitativeBiology\",\n",
    "                                            \"E.QuantitativeFinance\", \"F.Statistics\",\n",
    "                                            \"G.ElectricalEngineeringandSystemsScience\", \"H.Economics\"]),\n",
    "            LLMGuidedOptions(guided_regex=\"\", guided_choice=[\"A\", \"B\", \"C\", \"D\"]),\n",
    "        ]\n",
    "        for q_idx in range(3):\n",
    "            prompts = []\n",
    "            for i in range(num):\n",
    "                q = qs_list[i][q_idx]\n",
    "                conv_list[i].append_message(conv_list[i].roles[0], q)\n",
    "                conv_list[i].append_message(conv_list[i].roles[1], None)\n",
    "                prompt = conv_list[i].get_prompt()\n",
    "                prompts.append(TextPrompt(prompt=prompt, multi_modal_data={\"image\": image_list[i]}))\n",
    "\n",
    "            outputs = self.llm.generate(prompts=prompts, sampling_params=sampling_params, use_tqdm=True,\n",
    "                                        guided_options_request=guided_request[q_idx])\n",
    "\n",
    "            for i in range(num):\n",
    "                o = outputs[i]\n",
    "                text = o.outputs[0].text\n",
    "                ans_list[q_idx].append(text)\n",
    "                conv_list[i].messages[-1][-1] = text\n",
    "\n",
    "        return list(zip(*ans_list))"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "id": "efc4a4748f9707a5",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "worker = Worker()\n",
    "worker.run()"
   ],
   "execution_count": 8,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
