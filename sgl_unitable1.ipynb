{
 "cells": [
  {
   "id": "9e4c9e8a2c56bb3f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pkgs_path = \"/bohr/pkgs-7x29/v18/pkgs\"\n",
    "# llava_lib_path = \"/bohr/libb-bg5b/v3/llava\"\n",
    "# tsr_model_path = \"microsoft/table-structure-recognition-v1.1-all\"\n",
    "model_path = \"lmms-lab/llava-onevision-qwen2-7b-si\"\n",
    "cache_path = \"/bohr/cach-rxl3/v9/cache\"\n",
    "unitable_model = \"/bohr/unii-7sxm/v1/unitable/weights\"\n",
    "unitable_vocab = \"/bohr/unii-7sxm/v1/unitable/vocab\"\n",
    "unitable_src = \"/bohr/unii-7sxm/v1/unitable/src\"\n",
    "# pkgs_path = \"/personal/pkgs\"\n",
    "# llava_lib_path = \"/personal/llava\"\n",
    "# model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "# cache_path = \"/personal/cache\"\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.system(f\"pip install {pkgs_path}/* --ignore-installed\")\n",
    "os.system(f\"cp -r {unitable_src} .\")\n",
    "# os.system(f\"cp -r {llava_lib_path} .\")\n",
    "# # 提交时可能不能联网，设置成离线模式防止联网失败报错\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "device = \"cuda\""
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "id": "664dfe51317d5d0f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import json\n",
    "\n",
    "import torch.multiprocessing as multiprocessing\n",
    "\n",
    "from sglang.srt.server import launch_server\n",
    "from sglang.srt.server_args import ServerArgs\n",
    "from sglang.srt.utils import allocate_init_ports\n",
    "from sglang import RuntimeEndpoint\n",
    "\n",
    "import sglang as sgl\n",
    "from sglang.lang.chat_template import get_chat_template\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Sequence, Optional, Union\n",
    "\n",
    "import tokenizers as tk\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn, Tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.model import EncoderDecoder, ImgLinearBackbone, Encoder, Decoder\n",
    "from src.utils import subsequent_mask, pred_token_within_range, greedy_sampling, bbox_str_to_token_list, \\\n",
    "    cell_str_to_token_list, html_str_to_token_list, build_table_from_html_and_cell\n",
    "from src.vocab import (\n",
    "    HTML_TOKENS,\n",
    "    TASK_TOKENS,\n",
    "    RESERVED_TOKENS,\n",
    "    BBOX_TOKENS,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# import logging\n",
    "\n",
    "# multiprocessing.log_to_stderr(logging.INFO)\n",
    "# logger = multiprocessing.get_logger()"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "id": "3b21a43f-6086-4da8-857c-5db5ce2ef37a",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "id": "935eb4200002575e",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "if os.environ.get('DATA_PATH_B'):\n",
    "    base_dir = os.environ.get('DATA_PATH_B')\n",
    "else:\n",
    "    base_dir = '/bohr/form-recognition-train-b6y2/v4'\n",
    "with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "    data = list(json.load(f))\n",
    "    # data = data[:10]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "7d60f859a5c4cd7f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "class Runtime(sgl.srt.server.Runtime):\n",
    "    def __init__(\n",
    "            self,\n",
    "            log_level: str = \"error\",\n",
    "            model_overide_args: Optional[dict] = None,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        \"\"\"See the arguments in server_args.py::ServerArgs\"\"\"\n",
    "        self.server_args = ServerArgs(*args, log_level=log_level, **kwargs)\n",
    "\n",
    "        # Pre-allocate ports\n",
    "        self.server_args.port, self.server_args.additional_ports = allocate_init_ports(\n",
    "            self.server_args.port,\n",
    "            self.server_args.additional_ports,\n",
    "            self.server_args.dp_size,\n",
    "        )\n",
    "\n",
    "        self.url = self.server_args.url()\n",
    "        self.generate_url = (\n",
    "            f\"http://{self.server_args.host}:{self.server_args.port}/generate\"\n",
    "        )\n",
    "\n",
    "        self.pid = None\n",
    "        # logger.info(\"Launching server...\")\n",
    "        pipe_reader, pipe_writer = multiprocessing.Pipe(duplex=False)\n",
    "        proc = multiprocessing.Process(\n",
    "            target=launch_server,\n",
    "            args=(self.server_args, model_overide_args, pipe_writer),\n",
    "        )\n",
    "        # logger.info(\"Waiting for server to launch...\")\n",
    "        proc.start()\n",
    "        self.pid = proc.pid\n",
    "        # logger.info(\"Waiting for server to launch...\")\n",
    "        # pipe_writer.close()\n",
    "        # timeout = 60\n",
    "        # import time\n",
    "        # start_time = time.time()\n",
    "        #\n",
    "        # while True:\n",
    "        #     logger.info(\"Waiting for initialization state...\", flush=True)\n",
    "        #     if pipe_reader.poll(timeout=1):\n",
    "        #         logger.info(\"Waiting for initialization state...\", flush=True)\n",
    "        #         init_state = pipe_reader.recv()\n",
    "        #         break\n",
    "        #     if time.time() - start_time > timeout:\n",
    "        #         raise TimeoutError(\"Timeout while waiting for initialization state\")\n",
    "        # try:\n",
    "        #     init_state = pipe_reader.recv()\n",
    "        # except EOFError:\n",
    "        #     init_state = \"\"\n",
    "        init_state = pipe_reader.recv()\n",
    "\n",
    "        if init_state != \"init ok\":\n",
    "            self.shutdown()\n",
    "            raise RuntimeError(\n",
    "                \"Initialization failed. Please see the error messages above.\"\n",
    "            )\n",
    "        self.endpoint = RuntimeEndpoint(self.url)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "926ffd3102503a4e",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "MODEL_DIR = Path(unitable_model)\n",
    "MODEL_FILE_NAME = [\"unitable_large_structure.pt\", \"unitable_large_bbox.pt\", \"unitable_large_content.pt\"]\n",
    "VOCAB_DIR = Path(unitable_vocab)\n",
    "VOCAB_HTML = VOCAB_DIR / \"vocab_html.json\"\n",
    "VOCAB_BBOX = VOCAB_DIR / \"vocab_bbox.json\"\n",
    "VOCAB_CELL = VOCAB_DIR / \"vocab_cell_6k.json\"\n",
    "\n",
    "VALID_HTML_TOKEN = [\"<eos>\"] + HTML_TOKENS\n",
    "INVALID_CELL_TOKEN = ([\"<sos>\", \"<pad>\", \"<empty>\", \"<sep>\"] + TASK_TOKENS + RESERVED_TOKENS)\n",
    "VALID_BBOX_TOKEN = [\"<eos>\"] + BBOX_TOKENS  # image size will be addressed after instantiation\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# UniTable large model\n",
    "d_model = 768\n",
    "patch_size = 16\n",
    "nhead = 12\n",
    "dropout = 0.2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "2c711d711a809f62",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def autoregressive_decode(\n",
    "        model: EncoderDecoder,\n",
    "        image: Tensor,\n",
    "        prefix: Sequence[int],\n",
    "        max_decode_len: int,\n",
    "        eos_id: int,\n",
    "        token_whitelist: Optional[Sequence[int]] = None,\n",
    "        token_blacklist: Optional[Sequence[int]] = None,\n",
    ") -> Tensor:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        memory = model.encode(image)\n",
    "        context = torch.tensor(prefix, dtype=torch.int32).repeat(image.shape[0], 1).to(device)\n",
    "\n",
    "    for _ in range(max_decode_len):\n",
    "        eos_flag = [eos_id in k for k in context]\n",
    "        if all(eos_flag):\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            causal_mask = subsequent_mask(context.shape[1]).to(device)\n",
    "            logits = model.decode(\n",
    "                memory, context, tgt_mask=causal_mask, tgt_padding_mask=None\n",
    "            )\n",
    "            logits = model.generator(logits)[:, -1, :]\n",
    "\n",
    "        logits = pred_token_within_range(\n",
    "            logits.detach(),\n",
    "            white_list=token_whitelist,\n",
    "            black_list=token_blacklist,\n",
    "        )\n",
    "\n",
    "        next_probs, next_tokens = greedy_sampling(logits)\n",
    "        context = torch.cat([context, next_tokens], dim=1)\n",
    "    return context\n",
    "\n",
    "\n",
    "def load_vocab_and_model(\n",
    "        vocab_path: Union[str, Path],\n",
    "        max_seq_len: int,\n",
    "        model_weights: Union[str, Path],\n",
    "        backbone: nn.Module,\n",
    "        encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    ") -> Tuple[tk.Tokenizer, EncoderDecoder]:\n",
    "    vocab_path = str(vocab_path)\n",
    "    vocab = tk.Tokenizer.from_file(vocab_path)\n",
    "    model = EncoderDecoder(\n",
    "        backbone=backbone,\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        vocab_size=vocab.get_vocab_size(),\n",
    "        d_model=d_model,\n",
    "        padding_idx=vocab.token_to_id(\"<pad>\"),\n",
    "        max_seq_len=max_seq_len,\n",
    "        dropout=dropout,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(model_weights, map_location=\"cpu\"))\n",
    "    model = model.to(device)\n",
    "    return vocab, model\n",
    "\n",
    "\n",
    "def image_to_tensor(image: Image, size: Tuple[int, int]) -> Tensor:\n",
    "    T = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.86597056, 0.88463002, 0.87491087],\n",
    "            std=[0.20686628, 0.18201602, 0.18485524])\n",
    "    ])\n",
    "    image_tensor = T(image)\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)\n",
    "\n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "def rescale_bbox(\n",
    "        bbox: Sequence[Sequence[float]],\n",
    "        src: Tuple[int, int],\n",
    "        tgt: Tuple[int, int]\n",
    ") -> Sequence[Sequence[float]]:\n",
    "    assert len(src) == len(tgt) == 2\n",
    "    ratio = [tgt[0] / src[0], tgt[1] / src[1]] * 2\n",
    "    bbox = [[int(round(i * j)) for i, j in zip(entry, ratio)] for entry in bbox]\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def count_rows_and_columns(html_tags):\n",
    "    rows = 0\n",
    "    max_columns = 0\n",
    "    current_columns = 0\n",
    "    rowspan_columns = {}\n",
    "    index = 0\n",
    "    columns_cnt = defaultdict(int)\n",
    "    while index < len(html_tags):\n",
    "        tag = html_tags[index]\n",
    "\n",
    "        if tag == '<tr>':\n",
    "            rows += 1\n",
    "            current_columns = 0\n",
    "\n",
    "            # Account for any ongoing rowspans from previous rows\n",
    "            for col, span in rowspan_columns.items():\n",
    "                if span > 1:\n",
    "                    current_columns += 1\n",
    "                    rowspan_columns[col] -= 1\n",
    "\n",
    "        elif tag.startswith('<td'):\n",
    "            colspan = 1\n",
    "            rowspan = 1\n",
    "\n",
    "            # Check if 'colspan' and 'rowspan' are in the subsequent strings\n",
    "            if index + 1 < len(html_tags) and 'colspan=\"' in html_tags[index + 1]:\n",
    "                colspan = int(html_tags[index + 1].strip().split('colspan=\"')[1].split('\"')[0])\n",
    "                index += 1  # Skip the colspan string\n",
    "            if index + 1 < len(html_tags) and 'rowspan=\"' in html_tags[index + 1]:\n",
    "                rowspan = int(html_tags[index + 1].strip().split('rowspan=\"')[1].split('\"')[0])\n",
    "                index += 1  # Skip the rowspan string\n",
    "\n",
    "            # Increment columns count\n",
    "            current_columns += colspan\n",
    "\n",
    "            # Track rowspans for subsequent rows\n",
    "            if rowspan > 1:\n",
    "                for _ in range(colspan):\n",
    "                    rowspan_columns[current_columns - _] = rowspan\n",
    "\n",
    "        elif tag == '</tr>':\n",
    "            # print(f\"Row {rows} has {current_columns} columns\")\n",
    "            columns_cnt[current_columns] += 1\n",
    "            max_columns = max(max_columns, current_columns)\n",
    "\n",
    "        index += 1\n",
    "    columns = max(columns_cnt, key=columns_cnt.get)\n",
    "    return rows, columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "81ff31e099200a8",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "class Unitable:\n",
    "    def __init__(self):\n",
    "        manager = multiprocessing.Manager()\n",
    "        self.html = manager.list()\n",
    "        self.bbox = manager.list()\n",
    "        self.cell_input = multiprocessing.Queue()\n",
    "        self.shape = manager.list()\n",
    "        self.result = multiprocessing.Queue()\n",
    "\n",
    "    def run(self):\n",
    "        ps = [\n",
    "            multiprocessing.Process(target=self.img_tsr),\n",
    "            multiprocessing.Process(target=self.img_bbox),\n",
    "            multiprocessing.Process(target=self.img_tcr),\n",
    "        ]\n",
    "        for p in ps:\n",
    "            p.start()\n",
    "        for p in ps:\n",
    "            p.join()\n",
    "\n",
    "    def get(self):\n",
    "        return self.result.get()\n",
    "\n",
    "    def size(self):\n",
    "        return self.result.qsize()\n",
    "\n",
    "        # def __call__(self, image):\n",
    "\n",
    "    #     html = self.img_tsr(image)\n",
    "    #     bbox = self.img_bbox(image)\n",
    "    #     cell = self.img_tcr(image, bbox)\n",
    "    #     code = self.img2html(html, cell)\n",
    "    #     image = self.draw_bbox(image, bbox)\n",
    "    #     rows, cols = count_rows_and_columns(html)\n",
    "    #     print(\"HTML:\", html, flush=True)\n",
    "    #     print(\"BBOX:\", bbox, flush=True)\n",
    "    #     print(\"CODE:\", code, flush=True)\n",
    "    #     print(\"ROWS, COLS:\", rows, cols, flush=True)\n",
    "    #     return image, code, rows, cols\n",
    "    # return self.img_tsr(image)\n",
    "\n",
    "    def img_tsr(self):\n",
    "        backbone = ImgLinearBackbone(d_model=d_model, patch_size=patch_size)\n",
    "        encoder = Encoder(d_model=d_model, nhead=nhead, dropout=dropout, activation=\"gelu\",\n",
    "                          norm_first=True, nlayer=12, ff_ratio=4)\n",
    "        decoder = Decoder(d_model=d_model, nhead=nhead, dropout=dropout, activation=\"gelu\",\n",
    "                          norm_first=True, nlayer=4, ff_ratio=4)\n",
    "        vocab_html, model_html = load_vocab_and_model(\n",
    "            vocab_path=VOCAB_HTML,\n",
    "            max_seq_len=784,\n",
    "            model_weights=MODEL_DIR / MODEL_FILE_NAME[0],\n",
    "            backbone=backbone,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder\n",
    "        )\n",
    "\n",
    "        for item in data:\n",
    "            image = Image.open(os.path.join(base_dir, \"test_images\", item[\"image_path\"])).convert(\"RGB\")\n",
    "            # Image transformation\n",
    "            image_tensor = image_to_tensor(image, size=(448, 448))\n",
    "\n",
    "            # Inference\n",
    "            pred_html = autoregressive_decode(\n",
    "                model=model_html,\n",
    "                image=image_tensor,\n",
    "                prefix=[vocab_html.token_to_id(\"[html]\")],\n",
    "                max_decode_len=512,\n",
    "                eos_id=vocab_html.token_to_id(\"<eos>\"),\n",
    "                token_whitelist=[vocab_html.token_to_id(i) for i in VALID_HTML_TOKEN],\n",
    "                token_blacklist=None\n",
    "            )\n",
    "\n",
    "            # Convert token id to token text\n",
    "            pred_html = pred_html.detach().cpu().numpy()[0]\n",
    "            pred_html = vocab_html.decode(pred_html, skip_special_tokens=False)\n",
    "            pred_html = html_str_to_token_list(pred_html)\n",
    "\n",
    "            rows, cols = count_rows_and_columns(pred_html)\n",
    "            self.shape.append((rows, cols))\n",
    "            if self.bbox:\n",
    "                self.cell_input.put((pred_html, self.bbox.pop(0)))\n",
    "            else:\n",
    "                self.html.append(pred_html)\n",
    "        if not self.html and not self.bbox:\n",
    "            self.cell_input.put(None)\n",
    "\n",
    "    def img_bbox(self):\n",
    "        backbone = ImgLinearBackbone(d_model=d_model, patch_size=patch_size)\n",
    "        encoder = Encoder(d_model=d_model, nhead=nhead, dropout=dropout, activation=\"gelu\",\n",
    "                          norm_first=True, nlayer=12, ff_ratio=4)\n",
    "        decoder = Decoder(d_model=d_model, nhead=nhead, dropout=dropout, activation=\"gelu\",\n",
    "                          norm_first=True, nlayer=4, ff_ratio=4)\n",
    "        vocab_bbox, model_bbox = load_vocab_and_model(\n",
    "            vocab_path=VOCAB_BBOX,\n",
    "            max_seq_len=1024,\n",
    "            model_weights=MODEL_DIR / MODEL_FILE_NAME[1],\n",
    "            backbone=backbone,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder\n",
    "        )\n",
    "        for item in data:\n",
    "            image = Image.open(os.path.join(base_dir, \"test_images\", item[\"image_path\"])).convert(\"RGB\")\n",
    "\n",
    "            # Image transformation\n",
    "            image_tensor = image_to_tensor(image, size=(448, 448))\n",
    "            image_size = image.size\n",
    "            # Inference\n",
    "            pred_bbox = autoregressive_decode(\n",
    "                model=model_bbox,\n",
    "                image=image_tensor,\n",
    "                prefix=[vocab_bbox.token_to_id(\"[bbox]\")],\n",
    "                max_decode_len=1024,\n",
    "                eos_id=vocab_bbox.token_to_id(\"<eos>\"),\n",
    "                token_whitelist=[vocab_bbox.token_to_id(i) for i in VALID_BBOX_TOKEN[: 449]],\n",
    "                token_blacklist=None\n",
    "            )\n",
    "\n",
    "            # Convert token id to token text\n",
    "            pred_bbox = pred_bbox.detach().cpu().numpy()[0]\n",
    "            pred_bbox = vocab_bbox.decode(pred_bbox, skip_special_tokens=False)\n",
    "            pred_bbox = bbox_str_to_token_list(pred_bbox)\n",
    "            pred_bbox = rescale_bbox(pred_bbox, src=(448, 448), tgt=image_size)\n",
    "            if self.html:\n",
    "                self.cell_input.put((self.html.pop(0), pred_bbox))\n",
    "            else:\n",
    "                self.bbox.append(pred_bbox)\n",
    "        if not self.html and not self.bbox:\n",
    "            self.cell_input.put(None)\n",
    "\n",
    "    def img_tcr(self):\n",
    "        backbone = ImgLinearBackbone(d_model=d_model, patch_size=patch_size)\n",
    "        encoder = Encoder(d_model=d_model, nhead=nhead, dropout=dropout, activation=\"gelu\",\n",
    "                          norm_first=True, nlayer=12, ff_ratio=4)\n",
    "        decoder = Decoder(d_model=d_model, nhead=nhead, dropout=dropout, activation=\"gelu\",\n",
    "                          norm_first=True, nlayer=4, ff_ratio=4)\n",
    "        vocab_cell, model_cell = load_vocab_and_model(\n",
    "            vocab_path=VOCAB_CELL,\n",
    "            max_seq_len=200,\n",
    "            model_weights=MODEL_DIR / MODEL_FILE_NAME[2],\n",
    "            backbone=backbone,\n",
    "            encoder=encoder,\n",
    "            decoder=decoder\n",
    "        )\n",
    "        idx = 0\n",
    "        while True:\n",
    "            item = self.cell_input.get()\n",
    "            if item is None:\n",
    "                break\n",
    "            html, bbox = item\n",
    "            image = Image.open(os.path.join(base_dir, \"test_images\", data[idx][\"image_path\"])).convert(\"RGB\")\n",
    "\n",
    "            # Cell image cropping and transformation\n",
    "            image_tensor = [image_to_tensor(image.crop(b), size=(112, 448)) for b in bbox]\n",
    "            image_tensor = torch.cat(image_tensor, dim=0)\n",
    "\n",
    "            # Inference\n",
    "            pred_cell = autoregressive_decode(\n",
    "                model=model_cell,\n",
    "                image=image_tensor,\n",
    "                prefix=[vocab_cell.token_to_id(\"[cell]\")],\n",
    "                max_decode_len=200,\n",
    "                eos_id=vocab_cell.token_to_id(\"<eos>\"),\n",
    "                token_whitelist=None,\n",
    "                token_blacklist=[vocab_cell.token_to_id(i) for i in INVALID_CELL_TOKEN]\n",
    "            )\n",
    "\n",
    "            # Convert token id to token text\n",
    "            pred_cell = pred_cell.detach().cpu().numpy()\n",
    "            pred_cell = vocab_cell.decode_batch(pred_cell, skip_special_tokens=False)\n",
    "            pred_cell = [cell_str_to_token_list(i) for i in pred_cell]\n",
    "            pred_cell = [re.sub(r'(\\d).\\s+(\\d)', r'\\1.\\2', i) for i in pred_cell]\n",
    "\n",
    "            code = build_table_from_html_and_cell(html, pred_cell)\n",
    "            code = \"\".join(code)\n",
    "\n",
    "            self.result.put((idx, self.shape.pop(0), code))\n",
    "\n",
    "            # logger.info(f\"Image {idx} processed\")\n",
    "            # logger.info(f\"HTML: {html}\")\n",
    "            # logger.info(f\"BBOX: {bbox}\")\n",
    "            # logger.info(f\"CODE: {code}\")\n",
    "            idx += 1\n",
    "        self.result.put(None)\n",
    "\n",
    "    # def draw_bbox(self, image, bbox):\n",
    "    #     draw = ImageDraw.Draw(image)\n",
    "    #     for b in bbox:\n",
    "    #         draw.rectangle(b, outline=\"red\", width=1)\n",
    "    #     return image"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "490ec8f770a62d7e",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "@sgl.function\n",
    "def one_image(s, img_path, caption, q3, tsr):\n",
    "    q2 = \"\"\"Based on the table, caption and html, which subject is most relevant to the table and caption?\n",
    "A) Physics\n",
    "B) Mathematics\n",
    "C) Computer Science\n",
    "D) Quantitative Biology\n",
    "E) Quantitative Finance\n",
    "F) Statistics\n",
    "G) Electrical Engineering and Systems Science\n",
    "H) Economics\n",
    "\"\"\"\n",
    "    s += sgl.system(\n",
    "        \"You are a helpful assistant. Provide only an label ([A-H] or [A-D]) of the correct answer for multiple-choice questions.\")\n",
    "    s += sgl.user(\n",
    "        sgl.image(img_path) +\n",
    "        f'This is a table image. The caption of the table is \"{caption}\". The OCR recognition result of the table in HTML format is {tsr}, which can be used as a reference but no standard answer')\n",
    "    s += sgl.assistant(\"I have a general understanding of the information in this table.\")\n",
    "    s += sgl.user(q2)\n",
    "    s += sgl.assistant(\n",
    "        sgl.gen(\"subject\",\n",
    "                # choices=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"],\n",
    "                max_tokens=2, temperature=0.0, top_p=1, ignore_eos=True))\n",
    "    s += sgl.user(q3)\n",
    "    s += sgl.assistant(sgl.gen(\"option\",\n",
    "                               # choices=[\"A\", \"B\", \"C\", \"D\"],\n",
    "                               max_tokens=2, temperature=0.0, top_p=1, ignore_eos=True))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "ce8b4d4174b19252",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "class Worker:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.submission = []\n",
    "        self.ocr_data = multiprocessing.Queue()\n",
    "\n",
    "    def run(self):\n",
    "        ocr = multiprocessing.Process(target=self.ocr)\n",
    "        ocr.start()\n",
    "\n",
    "        model_overide_args = {\n",
    "            \"attn_implementation\": \"eager\",\n",
    "            \"multimodal\": True,\n",
    "            \"overwrite_config\": {\n",
    "                \"image_aspect_ratio\": \"anyres_max_9\"\n",
    "            }\n",
    "        }\n",
    "        runtime = Runtime(\n",
    "            model_path=model_path,\n",
    "            model_overide_args=model_overide_args,\n",
    "            load_balance_method=\"shortest_queue\",\n",
    "            enable_torch_compile=True\n",
    "        )\n",
    "        runtime.endpoint.chat_template = get_chat_template(\"qwen\")\n",
    "        sgl.set_default_backend(runtime)\n",
    "        self.process()\n",
    "        runtime.shutdown()\n",
    "        # ocr.join()\n",
    "\n",
    "    def ocr(self):\n",
    "        unitable = Unitable()\n",
    "        sub = multiprocessing.Process(target=unitable.run)\n",
    "        sub.start()\n",
    "        flag = True\n",
    "        while flag:\n",
    "            ocr_result = unitable.get()\n",
    "            if ocr_result is None:\n",
    "                break\n",
    "            size = min(unitable.size(), self.batch_size)\n",
    "            batch_result = [ocr_result]\n",
    "            for _ in range(size):\n",
    "                batch_result.append(unitable.get())\n",
    "            if batch_result[-1] is None:\n",
    "                batch_result.pop()\n",
    "                flag = False\n",
    "            batch_output = []\n",
    "            batch_data = []\n",
    "            for idx, shape, code in batch_result:\n",
    "                item = data[idx]\n",
    "                path = os.path.join(base_dir, \"test_images\", item[\"image_path\"])\n",
    "                question = item[\"question\"]\n",
    "                question = question[0].lower() + question[1:]\n",
    "                q3 = f\"\"\"Based on the table, caption and html, {question}\n",
    "A) {item[\"options\"][0]}\n",
    "B) {item[\"options\"][1]}\n",
    "C) {item[\"options\"][2]}\n",
    "D) {item[\"options\"][3]}\n",
    "\"\"\"\n",
    "                batch_output.append((item[\"image_path\"], shape))\n",
    "                batch_data.append({\n",
    "                    \"img_path\": path,\n",
    "                    \"caption\": item[\"caption\"],\n",
    "                    \"tsr\": code,\n",
    "                    \"q3\": q3,\n",
    "                })\n",
    "            self.ocr_data.put((batch_output, batch_data))\n",
    "        self.ocr_data.put(None)\n",
    "\n",
    "    def process(self):\n",
    "        while True:\n",
    "            item = self.ocr_data.get()\n",
    "            if item is None:\n",
    "                break\n",
    "            outputs, datas = item\n",
    "            states = one_image.run_batch(datas)\n",
    "            for o, s in zip(outputs, states):\n",
    "                self.clean_out(o, s)\n",
    "        # raise NotImplementedError\n",
    "        with open('submission.json', 'w') as f:\n",
    "            json.dump(self.submission, f)\n",
    "\n",
    "    def clean_out(self, o, s):\n",
    "        img_path, (rows, cols) = o\n",
    "        category = \"\"\n",
    "        answer = -1\n",
    "        try:\n",
    "            subject = s[\"subject\"]\n",
    "            match = re.search(r'[A-Za-z]', subject)\n",
    "            if match:\n",
    "                category = match.group(0).upper()\n",
    "                category = sub_list[l2i[category]]\n",
    "        except:\n",
    "            category = \"\"\n",
    "        try:\n",
    "            option = s[\"option\"]\n",
    "            match = re.search(r'[A-Za-z]', option)\n",
    "            if match:\n",
    "                answer = match.group(0).upper()\n",
    "                answer = l2i[answer]\n",
    "        except:\n",
    "            answer = -1\n",
    "        sub_item = {\n",
    "            \"image_path\": img_path,\n",
    "            \"category\": category,\n",
    "            \"cols\": cols,\n",
    "            \"rows\": rows,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "        self.submission.append(sub_item)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "id": "c1b896614c8ead1f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "worker = Worker()\n",
    "worker.run()"
   ],
   "execution_count": 6,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
