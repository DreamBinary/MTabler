{
 "cells": [
  {
   "id": "9e4c9e8a2c56bb3f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# config env\n",
    "pkgs_path = \"/bohr/pkgs-7x29/v13/pkgs\"\n",
    "llava_lib_path = \"/bohr/libb-bg5b/v3/llava\"\n",
    "tsr_model_path = \"microsoft/table-structure-recognition-v1.1-all\"\n",
    "\n",
    "help_model_path = \"OpenGVLab/InternVL2-2B\"\n",
    "main_model_path = \"lmms-lab/llava-onevision-qwen2-7b-si\"\n",
    "cache_path = \"/bohr/cach-rxl3/v7/cache\"\n",
    "\n",
    "# pkgs_path = \"/personal/pkgs\"\n",
    "# llava_lib_path = \"/personal/llava\"\n",
    "# model_path = \"lmms-lab/llava-onevision-qwen2-0.5b-ov\"\n",
    "# cache_path = \"/personal/cache\"\n",
    "\n",
    "import os\n",
    "\n",
    "os.system(f\"pip install --no-index --find-links={pkgs_path}  --ignore-installed {pkgs_path}/*\")\n",
    "os.system(f\"cp {llava_lib_path} . -r\")\n",
    "\n",
    "# # 提交时可能不能联网，设置成离线模式防止联网失败报错\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "device = \"cuda\""
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "id": "664dfe51317d5d0f",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX\n",
    "\n",
    "from llava.conversation import Conversation, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "import json\n",
    "from llava.constants import DEFAULT_IMAGE_TOKEN\n",
    "import torch\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import AutoImageProcessor, TableTransformerForObjectDetection\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from vllm import LLM, SamplingParams, TextPrompt\n",
    "import multiprocessing\n",
    "from vllm.model_executor.guided_decoding.guided_fields import LLMGuidedOptions\n"
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "id": "89f8307aac1c3988",
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-07T11:57:07.389264Z",
     "start_time": "2024-08-07T11:57:07.273027Z"
    }
   },
   "source": [
    "args = type('Args', (), {\n",
    "    \"conv_mode\": None,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 8\n",
    "})()\n",
    "\n",
    "l2i = defaultdict(lambda: -1)\n",
    "for i, letter in enumerate('ABCDEFGH'):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "disable_torch_init()"
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "id": "54835e82f238325c",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if os.environ.get('DATA_PATH_B'):  # 提交时会选择隐藏的测试数据集路径（A+B榜），数据集的格式与A榜数据相同，但数目不同（5360张）\n",
    "    base_dir = os.environ.get('DATA_PATH_B')\n",
    "else:\n",
    "    base_dir = '/bohr/form-recognition-train-b6y2/v4'  # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug   # 示例，把A榜测试数据集路径作为测试集路径，仅开发时挂载A榜数据用于debug"
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "id": "ce8b4d4174b19252",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def clean_out(image_path, out_list):\n",
    "    matches = re.findall(r\"\\d+\", out_list[0])\n",
    "    if len(matches) >= 2:\n",
    "        rows, cols = int(matches[0]), int(matches[1])\n",
    "    elif len(matches) == 1:\n",
    "        rows = cols = int(matches[0])\n",
    "    else:\n",
    "        rows = cols = -1\n",
    "\n",
    "    sub_item = {\n",
    "        \"image_path\": image_path,\n",
    "        \"category\": sub_list[l2i[out_list[1][0]]],\n",
    "        \"cols\": cols,\n",
    "        \"rows\": rows,\n",
    "        \"answer\": l2i[out_list[2][0]],\n",
    "    }\n",
    "    return sub_item"
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "id": "27df6c9dcc02c500",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# class Generator(SequenceGeneratorAdapter):\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__(model, None, multinomial())\n",
    "# \n",
    "#     def generate(\n",
    "#             self,\n",
    "#             prompts: Union[str, List[str]],\n",
    "#             logits_processor: FSMLogitsProcessor,\n",
    "#             max_tokens: Optional[int] = None,\n",
    "#             stop_at: Optional[Union[str, List[str]]] = None,\n",
    "#             seed: Optional[int] = None,\n",
    "#             **model_specific_params\n",
    "#     ):\n",
    "#         generation_params = self.prepare_generation_parameters(\n",
    "#             max_tokens, stop_at, seed\n",
    "#         )\n",
    "# \n",
    "#         completions = self.model.generate(\n",
    "#             prompts,\n",
    "#             generation_params,\n",
    "#             logits_processor,\n",
    "#             self.sampling_params,\n",
    "#             **model_specific_params,\n",
    "#         )\n",
    "# \n",
    "#         return self._format(completions)"
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "id": "43afc4bd-68cb-4ed9-a0f2-dd14d74d1e26",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Worker:\n",
    "    def __init__(self):\n",
    "        with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "            # self.data = list(json.load(f))[:2]\n",
    "        # manager = multiprocessing.Manager()\n",
    "        self.tsr_result = multiprocessing.Queue()\n",
    "        # self.help_result = manager.list()\n",
    "        self.main_input = multiprocessing.Queue()\n",
    "\n",
    "    def run(self):\n",
    "        # multiprocessing.set_start_method('spawn')\n",
    "        help_process = multiprocessing.Process(target=self.help_process, daemon=True)\n",
    "        tsr_process = multiprocessing.Process(target=self.tsr_process, daemon=True)\n",
    "        tsr_process.start()\n",
    "        self.tokenizer, self.model, self.image_processor, _ = load_pretrained_model(\n",
    "            main_model_path, None, \"llava_qwen\", device_map=\"auto\",\n",
    "            attn_implementation='sdpa',\n",
    "            # load_8bit=True,\n",
    "            # load_4bit=False,\n",
    "            **{\n",
    "                \"multimodal\": True,\n",
    "                \"overwrite_config\": {\n",
    "                    \"image_aspect_ratio\": \"anyres_max_9\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        help_process.start()\n",
    "\n",
    "        self.main_process()\n",
    "        # tsr_process.join()\n",
    "        # help_process.join()\n",
    "        # print(\"RUN END\")\n",
    "\n",
    "    def tsr_process(self):\n",
    "        tsr_img_processor = AutoImageProcessor.from_pretrained(tsr_model_path)\n",
    "        tsr_img_processor.size = {'height': 384, 'width': 384}\n",
    "        tsr_model = TableTransformerForObjectDetection.from_pretrained(tsr_model_path)\n",
    "        label2id = tsr_model.config.label2id\n",
    "        label_row = label2id['table row']\n",
    "        label_col = label2id['table column']\n",
    "        for item in self.data:\n",
    "            path = os.path.join(base_dir, 'test_images', item[\"image_path\"])\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            inputs = tsr_img_processor(images=image, return_tensors=\"pt\")\n",
    "            outputs = tsr_model(**inputs)\n",
    "\n",
    "            target_sizes = torch.tensor([image.size[::-1]])  # (height, width) of each image in the batch\n",
    "            results = \\\n",
    "                tsr_img_processor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[0]\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            rows = 0\n",
    "            cols = 0\n",
    "            for label, box in zip(results[\"labels\"], results[\"boxes\"]):\n",
    "                label, box = label.item(), box.tolist()\n",
    "                draw.rectangle(box, outline=\"red\", width=1)\n",
    "                if label == label_row:\n",
    "                    rows += 1\n",
    "                elif label == label_col:\n",
    "                    cols += 1\n",
    "            self.tsr_result.put((image, rows, cols, item))\n",
    "            # if self.help_result:\n",
    "            #     self.main_input.put((self.help_result.pop(0), (image, rows, cols)))\n",
    "            # else:\n",
    "            #     self.tsr_result.append((image, rows, cols))\n",
    "            print(\"TSR\", rows, cols)\n",
    "            # print(\"-->> len_help\", len(self.help_result))\n",
    "        # if not self.help_result and not self.tsr_result:\n",
    "        #     self.main_input.put(None)\n",
    "        self.tsr_result.put(None)\n",
    "\n",
    "    def help_process(self):\n",
    "        try:\n",
    "            model = LLM(\n",
    "                model=help_model_path,\n",
    "                trust_remote_code=True,\n",
    "                dtype=\"float16\",\n",
    "                max_model_len=2048,\n",
    "                enforce_eager=True,\n",
    "                gpu_memory_utilization=0.5\n",
    "            )\n",
    "            tokenizer = model.get_tokenizer()\n",
    "            stop_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\", \"<|end|>\"]\n",
    "            stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n",
    "            flag = True\n",
    "            while flag:\n",
    "                tsr_item = self.tsr_result.get()\n",
    "                if tsr_item is None:\n",
    "                    break\n",
    "                size = self.tsr_result.qsize()\n",
    "                size = min(size, 4)\n",
    "                tsr_items = [tsr_item] + [self.tsr_result.get() for _ in range(size)]\n",
    "                if tsr_items[-1] is None:\n",
    "                    tsr_items = tsr_items[:-1]\n",
    "                    flag = False\n",
    "                images, rows, cols, items = map(list, zip(*tsr_items))\n",
    "                size = len(tsr_items)\n",
    "                qs_list = [\n",
    "                    [\n",
    "                        f'Based on the provided table, what is its shape? Answer with two positive integers for rows and columns.',\n",
    "                        f\"\"\"Based on the provided table and caption, select the most relevant subject from (A. Physics, B. Mathematics, C. ComputerScience, D. QuantitativeBiology, E. QuantitativeFinance, F. Statistics, G. ElectricalEngineeringandSystemsScience, H. Economics).\"\"\",\n",
    "                        f\"\"\"Based on the provided table and caption, for the question: \"{item[\"question\"]}\", select the most correct option from (A. {item[\"options\"][0]}, B. {item[\"options\"][1]}, C. {item[\"options\"][2]}, D. {item[\"options\"][3]}). Answer with the option's letter from the given choices directly.\"\"\"\n",
    "                    ] for item in items\n",
    "                ]\n",
    "                prefix = [\n",
    "                    [\n",
    "                        f'<image>\\n This is a table image with red borders. The table shape might be ({rows[i]}, {cols[i]}) but could vary.',\n",
    "                        f'<image>\\n This is a table image with red borders. The caption of the table is \"{items[i][\"caption\"]}\"',\n",
    "                        f'<image>\\n This is a table image with red borders. The caption of the table is \"{items[i][\"caption\"]}\"',\n",
    "                    ] for i in range(size)\n",
    "                ]\n",
    "                # messages = [\n",
    "                #     [\n",
    "                #         {'role': 'system',\n",
    "                #          'content': \"You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"},\n",
    "                #         {'role': 'user',\n",
    "                #          'content': f'<image>\\n This is a table image with red borders. The table shape might be ({rows[i]}, {cols[i]}) but could vary. The caption of the table is \"{items[i][\"caption\"]}\".'},\n",
    "                #         {'role': 'assistant',\n",
    "                #          'content': \"I have a general understanding of the information in this table.\"}\n",
    "                #     ] for i in range(size)\n",
    "                # ]\n",
    "                guided_request = [\n",
    "                    LLMGuidedOptions(guided_regex=r\"\\d,\\s*\\d\"),\n",
    "                    LLMGuidedOptions(\n",
    "                        guided_choice=[\"A.Physics\", \"B.Mathematics\", \"C.ComputerScience\", \"D.QuantitativeBiology\",\n",
    "                                       \"E.QuantitativeFinance\", \"F.Statistics\",\n",
    "                                       \"G.ElectricalEngineeringandSystemsScience\", \"H.Economics\"]),\n",
    "                    LLMGuidedOptions(guided_choice=[\"A\", \"B\", \"C\", \"D\"]),\n",
    "                ]\n",
    "                out_list = [[], [], []]\n",
    "                for q_idx in range(3):\n",
    "                    prompts = []\n",
    "                    for i in range(size):\n",
    "                        message = [{'role': 'user', 'content': f'{prefix[i][q_idx]} {qs_list[i][q_idx]}'}]\n",
    "                        prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "                        prompt = TextPrompt(prompt=prompt, multi_modal_data={\"image\": images[i]})\n",
    "                        prompts.append(prompt)\n",
    "                        # print(\"HELP PROMPT:\", prompt)\n",
    "                    # noinspection PyTypeChecker\n",
    "                    outputs = model.generate(prompts=prompts,\n",
    "                                             sampling_params=SamplingParams(\n",
    "                                                 temperature=0, max_tokens=64, stop_token_ids=stop_token_ids\n",
    "                                             ),\n",
    "                                             guided_options_request=guided_request[q_idx],\n",
    "                                             use_tqdm=False)\n",
    "                    for i, output in enumerate(outputs):\n",
    "                        text = output.outputs[0].text\n",
    "                        print(\"HELP OUT:\", text)\n",
    "                        out_list[q_idx].append(text)\n",
    "                out_list = list(zip(*out_list))\n",
    "                for i in range(size):\n",
    "                    self.main_input.put(\n",
    "                        (items[i], qs_list[i], out_list[i], images[i], rows[i], cols[i])\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            self.main_input.put(None)\n",
    "        self.main_input.put(None)\n",
    "\n",
    "    def main_process(self):\n",
    "        submission = []\n",
    "        while True:\n",
    "            item = self.main_input.get()\n",
    "            # print(\"MAIN ITEM\", item)\n",
    "            if item is None:\n",
    "                # if len(submission) < 4000:\n",
    "                #     sys.exit()\n",
    "                break\n",
    "\n",
    "            (item, qs_list, out_list, image, rows, cols) = item\n",
    "            image_sizes = [image.size]\n",
    "            images = [image]\n",
    "            image_tensors = [\n",
    "                process_images(images, self.image_processor, self.model.config)[0].to(dtype=torch.float16,\n",
    "                                                                                      device=device)]\n",
    "            conv = Conversation(\n",
    "                system=\"\"\"<|im_start|>system\n",
    "                        You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"\"\",\n",
    "                roles=[\"<|im_start|>user\", \"<|im_start|>assistant\"],\n",
    "                version=\"qwen\",\n",
    "                messages=[\n",
    "                    [\"<|im_start|>user\",\n",
    "                     f'{DEFAULT_IMAGE_TOKEN}\\n This is a table image with red borders. The table shape might be ({rows}, {cols}) but could vary. The caption of the table is \"{item[\"caption\"]}\". Besides that, for the following three questions, the answer from the other model is {out_list}, which you can use as a reference.'],\n",
    "                    [\"<|im_start|>assistant\", \"I have a general understanding of the information in this table.\"]\n",
    "                ],\n",
    "                offset=0,\n",
    "                sep_style=SeparatorStyle.CHATML,\n",
    "                sep=\"<|im_end|>\",\n",
    "            )\n",
    "            out_list = self.one_image(self.model, self.tokenizer, image_tensors, image_sizes, conv, qs_list)\n",
    "            sub_item = clean_out(item[\"image_path\"], out_list)\n",
    "            print(\"MAIN:\", out_list)\n",
    "            submission.append(sub_item)\n",
    "        with open('submission.json', 'w') as f:\n",
    "            json.dump(submission, f)\n",
    "\n",
    "    def one_image(self, model, tokenizer, image_tensors, image_sizes, conv, qs_list):\n",
    "        out_list = []\n",
    "        with torch.inference_mode():\n",
    "            for qs in qs_list:\n",
    "                # for qs, processor in zip(qs_list, logits_processor):\n",
    "                conv.append_message(conv.roles[0], qs)\n",
    "                conv.append_message(conv.roles[1], None)\n",
    "                prompt = conv.get_prompt()\n",
    "                input_ids = tokenizer_image_token(\n",
    "                    prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n",
    "\n",
    "                output_ids = model.generate(\n",
    "                    prompts=input_ids,\n",
    "                    images=image_tensors,\n",
    "                    image_sizes=image_sizes,\n",
    "                    do_sample=False,\n",
    "                    temperature=0,\n",
    "                    top_p=1,\n",
    "                    num_beams=1,\n",
    "                    max_new_tokens=8,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "                conv.messages[-1][-1] = outputs\n",
    "                out_list.append(outputs)\n",
    "        return out_list\n",
    "\n",
    "    # def main_process(self):\n",
    "    #     tokenizer, model, image_processor, _ = load_pretrained_model(\n",
    "    #         main_model_path, None, \"llava_qwen\", device_map=\"auto\",\n",
    "    #         attn_implementation='sdpa',\n",
    "    #         # load_8bit=True,\n",
    "    #         # load_4bit=False,\n",
    "    #         **{\n",
    "    #             \"multimodal\": True,\n",
    "    #             \"overwrite_config\": {\n",
    "    #                 \"image_aspect_ratio\": \"anyres_max_9\"\n",
    "    #             }\n",
    "    #         }\n",
    "    #     )\n",
    "    # \n",
    "    #     generator = Generator(model)\n",
    "    #     processor_list = [\n",
    "    #         RegexLogitsProcessor(r\"\\d,\\s*\\d\", tokenizer=tokenizer),\n",
    "    #         RegexLogitsProcessor(r\"A|B|C|D|E|F|G|H\", tokenizer=tokenizer),\n",
    "    #         RegexLogitsProcessor(r\"A|B|C|D\", tokenizer=tokenizer),\n",
    "    #     ]\n",
    "    #     submission = []\n",
    "    #     while True:\n",
    "    #         item = self.main_input.get()\n",
    "    #         # print(\"MAIN ITEM\", item)\n",
    "    #         if item is None:\n",
    "    #             if len(submission) < 4000:\n",
    "    #                 sys.exit()\n",
    "    #             break\n",
    "    # \n",
    "    #         (item, qs_list, out_list, image, rows, cols) = item\n",
    "    #         image_sizes = [image.size]\n",
    "    #         images = [image]\n",
    "    #         image_tensors = [\n",
    "    #             process_images(images, image_processor, model.config)[0].to(dtype=torch.float16, device=device)]\n",
    "    #         conv = Conversation(\n",
    "    #             system=\"\"\"<|im_start|>system\n",
    "    #                     You are a helpful assistant. Provide only an option's letter or an integer for each question, without any additional explanation.\"\"\",\n",
    "    #             roles=[\"<|im_start|>user\", \"<|im_start|>assistant\"],\n",
    "    #             version=\"qwen\",\n",
    "    #             messages=[\n",
    "    #                 [\"<|im_start|>user\",\n",
    "    #                  f'{DEFAULT_IMAGE_TOKEN}\\n This is a table image with red borders. The table shape might be ({rows}, {cols}) but could vary. The caption of the table is \"{item[\"caption\"]}\". Besides that, for the following three questions, the answer from the other model is {out_list}, which you can use as a reference.'],\n",
    "    #                 [\"<|im_start|>assistant\", \"I have a general understanding of the information in this table.\"]\n",
    "    #             ],\n",
    "    #             offset=0,\n",
    "    #             sep_style=SeparatorStyle.CHATML,\n",
    "    #             sep=\"<|im_end|>\",\n",
    "    #         )\n",
    "    #         out_list = self.one_image(generator, tokenizer, processor_list, image_tensors, image_sizes, conv, qs_list)\n",
    "    #         sub_item = clean_out(item[\"image_path\"], out_list)\n",
    "    #         # print(\"MAIN:\", out_list)\n",
    "    #         submission.append(sub_item)\n",
    "    #     with open('submission.json', 'w') as f:\n",
    "    #         json.dump(submission, f)\n",
    "\n",
    "    # def one_image(self, generator: Generator, tokenizer, logits_processor, image_tensors, image_sizes, conv, qs_list):\n",
    "    #     out_list = []\n",
    "    #     with torch.inference_mode():\n",
    "    #         # for qs in qs_list:\n",
    "    #         for qs, processor in zip(qs_list, logits_processor):\n",
    "    #             conv.append_message(conv.roles[0], qs)\n",
    "    #             conv.append_message(conv.roles[1], None)\n",
    "    #             prompt = conv.get_prompt()\n",
    "    #             input_ids = tokenizer_image_token(\n",
    "    #                 prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n",
    "    # \n",
    "    #             output_ids = generator.generate(\n",
    "    #                 prompts=input_ids,\n",
    "    #                 logits_processor=processor,\n",
    "    #                 images=image_tensors,\n",
    "    #                 image_sizes=image_sizes,\n",
    "    #                 do_sample=False,\n",
    "    #                 temperature=0,\n",
    "    #                 top_p=1,\n",
    "    #                 num_beams=1,\n",
    "    #                 max_new_tokens=8,\n",
    "    #                 use_cache=True,\n",
    "    #             )\n",
    "    #             outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "    #             conv.messages[-1][-1] = outputs\n",
    "    #             out_list.append(outputs)\n",
    "    #     return out_list"
   ],
   "execution_count": " ",
   "outputs": []
  },
  {
   "id": "efc4a4748f9707a5",
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "worker = Worker()\n",
    "worker.run()"
   ],
   "execution_count": " ",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
