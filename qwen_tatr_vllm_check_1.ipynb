{
 "cells": [
  {
   "id": "initial_id",
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "import json\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.model_executor.guided_decoding import GuidedDecodingRequest\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "opkgs_path = \"/bohr/opkgs-k2wz/v2/opkgs\"\n",
    "tatr_path = \"/bohr/tatr-xdh6/v2/tatr\"\n",
    "str_model_path = '/bohr/TATR-xmup/v1/TATR/TATR-v1.1-All-msft.pth'\n",
    "str_config_path = '/bohr/TATR-xmup/v1/TATR/structure_config.json'\n",
    "model_path = \"/bohr/cach-rxl3/v17/cache/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/51c47430f97dd7c74aa1fa6825e68a813478097f\"\n",
    "torch_hub_path = \"/bohr/thub-w4uy/v1\"\n",
    "cache_path = \"/bohr/cach-rxl3/v17/cache\"\n",
    "\n",
    "os.system(f\"pip3 install {opkgs_path}/*\")\n",
    "os.system(f\"cp -r {tatr_path} .\")\n",
    "# os.system(f\"cp -r {raw_cache_path} .\")\n",
    "# os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "# os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "# os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ[\"TORCH_HOME\"] = torch_hub_path\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = cache_path\n",
    "os.environ[\"HF_HOME\"] = cache_path\n",
    "device = \"cuda\"\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "l2i = defaultdict(lambda: -1)\n",
    "label = \"ABCDEFGH\"\n",
    "for i, letter in enumerate(label):\n",
    "    l2i[letter] = i\n",
    "sub_list = ('Physics', 'Mathematics', 'ComputerScience', 'QuantitativeBiology', 'QuantitativeFinance',\n",
    "            'Statistics', 'ElectricalEngineeringandSystemsScience', 'Economics', '')\n",
    "IMAGE_FACTOR = 28\n",
    "MIN_PIXELS = 4 * 28 * 28\n",
    "MAX_PIXELS = 16384 * 28 * 28\n",
    "MAX_RATIO = 200\n",
    "\n",
    "\n",
    "def round_by_factor(number: int, factor: int) -> int:\n",
    "    \"\"\"Returns the closest integer to 'number' that is divisible by 'factor'.\"\"\"\n",
    "    return round(number / factor) * factor\n",
    "\n",
    "\n",
    "def ceil_by_factor(number: int, factor: int) -> int:\n",
    "    \"\"\"Returns the smallest integer greater than or equal to 'number' that is divisible by 'factor'.\"\"\"\n",
    "    return math.ceil(number / factor) * factor\n",
    "\n",
    "\n",
    "def floor_by_factor(number: int, factor: int) -> int:\n",
    "    \"\"\"Returns the largest integer less than or equal to 'number' that is divisible by 'factor'.\"\"\"\n",
    "    return math.floor(number / factor) * factor\n",
    "\n",
    "\n",
    "def smart_resize(\n",
    "        height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS\n",
    "):\n",
    "    \"\"\"\n",
    "    Rescales the image so that the following conditions are met:\n",
    "\n",
    "    1. Both dimensions (height and width) are divisible by 'factor'.\n",
    "\n",
    "    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n",
    "\n",
    "    3. The aspect ratio of the image is maintained as closely as possible.\n",
    "    \"\"\"\n",
    "    if max(height, width) / min(height, width) > MAX_RATIO:\n",
    "        raise ValueError(\n",
    "            f\"absolute aspect ratio must be smaller than {MAX_RATIO}, got {max(height, width) / min(height, width)}\"\n",
    "        )\n",
    "    h_bar = max(factor, round_by_factor(height, factor))\n",
    "    w_bar = max(factor, round_by_factor(width, factor))\n",
    "    if h_bar * w_bar > max_pixels:\n",
    "        beta = math.sqrt((height * width) / max_pixels)\n",
    "        h_bar = floor_by_factor(height / beta, factor)\n",
    "        w_bar = floor_by_factor(width / beta, factor)\n",
    "    elif h_bar * w_bar < min_pixels:\n",
    "        beta = math.sqrt(min_pixels / (height * width))\n",
    "        h_bar = ceil_by_factor(height * beta, factor)\n",
    "        w_bar = ceil_by_factor(width * beta, factor)\n",
    "    return h_bar, w_bar\n",
    "\n",
    "\n",
    "option2 = [\n",
    "    \"Physics\",\n",
    "    \"Mathematics\",\n",
    "    \"Computer Science\",\n",
    "    \"Quantitative Biology\",\n",
    "    \"Quantitative Finance\",\n",
    "    \"Statistics\",\n",
    "    \"Electrical Engineering and Systems Science\",\n",
    "    \"Economics\",\n",
    "]\n",
    "\n",
    "if os.environ.get('DATA_PATH_B'):\n",
    "    base_dir = os.environ.get('DATA_PATH_B')\n",
    "    with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "        # data_t = list(json.load(f))[:100]\n",
    "else:\n",
    "    base_dir = '/bohr/form-recognition-train-b6y2/v4'\n",
    "    with open(os.path.join(base_dir, 'dataset.json'), 'r') as f:\n",
    "        raw_data = list(json.load(f))[:10]\n",
    "length = len(raw_data)\n",
    "ocr_data = multiprocessing.Manager().list()\n",
    "batch_size = 8\n",
    "tmp_ans2 = defaultdict(lambda: defaultdict(int))\n",
    "tmp_ans3 = defaultdict(lambda: defaultdict(int))\n",
    "final_ans2 = [-1] * length\n",
    "final_ans3 = [-1] * length\n",
    "placeholder = \"%%pl_ac_eh_+=ol_&der%%\"\n",
    "\n",
    "\n",
    "def shuffle(sou):\n",
    "    tag = sou.copy()\n",
    "    random.shuffle(tag)\n",
    "    order = [sou.index(x) for x in tag]\n",
    "    tag = [f\"{label[i]}) {x}\" for i, x in enumerate(tag)]\n",
    "    shuffled = \"\\n\".join(tag)\n",
    "    return shuffled, order\n",
    "\n",
    "\n",
    "def gen_inputs2(idxs):\n",
    "    inputs = []\n",
    "    orders = []\n",
    "    for i in idxs:\n",
    "        option, order = shuffle(option2)\n",
    "        q2 = ocr_data[i][\"q2\"].replace(placeholder, option)\n",
    "        inputs.append({\n",
    "            \"prompt\": q2,\n",
    "            \"multi_modal_data\": {\n",
    "                \"image\": ocr_data[i][\"img\"]\n",
    "            }\n",
    "        })\n",
    "        orders.append(order)\n",
    "    orders.append(-1)\n",
    "    return inputs, orders\n",
    "\n",
    "\n",
    "def gen_inputs3(idxs):\n",
    "    inputs = []\n",
    "    orders = []\n",
    "    for i in idxs:\n",
    "        option, order = shuffle(raw_data[i][\"options\"])\n",
    "        q3 = ocr_data[i][\"q3\"].replace(placeholder, option)\n",
    "        inputs.append({\n",
    "            \"prompt\": q3,\n",
    "            \"multi_modal_data\": {\n",
    "                \"image\": ocr_data[i][\"img\"]\n",
    "            }\n",
    "        })\n",
    "        orders.append(order)\n",
    "    return inputs, orders\n",
    "\n",
    "\n",
    "def process():\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        limit_mm_per_prompt={\"image\": 1},\n",
    "    )\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        top_p=1,\n",
    "        repetition_penalty=1.05,\n",
    "        max_tokens=4,\n",
    "        stop_token_ids=[],\n",
    "    )\n",
    "    guided_options_request = GuidedDecodingRequest(\n",
    "        guided_regex=r\"[A-Ha-h]\",\n",
    "    )\n",
    "\n",
    "    idx2, idx3 = 0, 0\n",
    "    batch_idx2, batch_idx3 = [], []\n",
    "    while True:\n",
    "        if len(batch_idx2) == 0 and len(batch_idx3) == 0 and idx2 >= length and idx3 >= length:\n",
    "            break\n",
    "        while len(batch_idx2) < batch_size and idx2 < length:\n",
    "            batch_idx2.append(idx2)\n",
    "            idx2 += 1\n",
    "        while len(batch_idx3) < batch_size and idx3 < length:\n",
    "            batch_idx3.append(idx3)\n",
    "            idx3 += 1\n",
    "        inputs2, orders2 = gen_inputs2(batch_idx2)\n",
    "        inputs3, orders3 = gen_inputs3(batch_idx3)\n",
    "\n",
    "        outputs = llm.generate(\n",
    "            inputs2 + inputs3,\n",
    "            sampling_params=sampling_params,\n",
    "            use_tqdm=False,\n",
    "            guided_options_request=guided_options_request\n",
    "        )\n",
    "        ans = [output.outputs[0].text for output in outputs]\n",
    "        ans = [clean_ans(a) for a in ans]\n",
    "        ans2, ans3 = ans[:len(inputs2)], ans[len(inputs2):]\n",
    "\n",
    "        t2, t3 = [], []\n",
    "        for i, a, o in zip(batch_idx2, ans2, orders2):\n",
    "            real_ans = o[a]\n",
    "            if tmp_ans2[i][real_ans] == 2:\n",
    "                final_ans2[i] = real_ans\n",
    "                t2.append(i)\n",
    "            else:\n",
    "                tmp_ans2[i][real_ans] += 1\n",
    "        for i, a, o in zip(batch_idx3, ans3, orders3):\n",
    "            real_ans = o[a]\n",
    "            if tmp_ans3[i][real_ans] == 2:\n",
    "                final_ans3[i] = real_ans\n",
    "                t3.append(i)\n",
    "            else:\n",
    "                tmp_ans3[i][real_ans] += 1\n",
    "        for i in t2:\n",
    "            batch_idx2.remove(i)\n",
    "        for i in t3:\n",
    "            batch_idx3.remove(i)\n",
    "\n",
    "\n",
    "def clean_ans(ans):\n",
    "    try:\n",
    "        match = re.search(r'[A-Ha-h]', ans)\n",
    "        if match:\n",
    "            return l2i[match.group(0).upper()]\n",
    "    except:\n",
    "        return -1\n",
    "    return -1\n",
    "\n",
    "\n",
    "def fetch_image(img, size_factor: int = IMAGE_FACTOR) -> Image.Image:\n",
    "    width, height = img.size\n",
    "    resized_height, resized_width = smart_resize(\n",
    "        height,\n",
    "        width,\n",
    "        factor=size_factor,\n",
    "        min_pixels=MIN_PIXELS,\n",
    "        max_pixels=MAX_PIXELS,\n",
    "    )\n",
    "    img = img.resize((resized_width, resized_height))\n",
    "    return img\n",
    "\n",
    "\n",
    "def ocr():\n",
    "    from tatr import TableEngine\n",
    "    engine = TableEngine(\n",
    "        str_device=device,\n",
    "        str_model_path=str_model_path,\n",
    "        str_config_path=str_config_path\n",
    "    )\n",
    "\n",
    "    template = \"\"\"<|im_start|>system\n",
    "{sys}<|im_end|>\n",
    "<|im_start|>user\n",
    "<|vision_start|><|image_pad|><|vision_end|>{q}<|im_end|>\n",
    "<|im_start|>assistant\"\"\"\n",
    "\n",
    "    q_prefix = \"Based on the table, caption and html structure, \"\n",
    "    sys2 = \"You are a helpful assistant. Provide only a label [A-H] of the correct answer for multiple-choice questions.\"\n",
    "    sys3 = \"You are a helpful assistant. Provide only a label [A-D] of the correct answer for multiple-choice questions.\"\n",
    "    for d in raw_data:\n",
    "        r_path = os.path.join(base_dir, \"test_images\", d[\"image_path\"])\n",
    "        img = Image.open(r_path).convert(\"RGB\")\n",
    "        html, rows, cols = engine(img)\n",
    "        q1 = f'This is a table image. The caption of the table is \"{d[\"caption\"]}\". The structure of the table in html format is as follows: {html}.'\n",
    "        q2 = f\"\"\"{q1}{q_prefix}which subject is most relevant to the table or caption?\\n{placeholder}\"\"\"\n",
    "        question = d[\"question\"]\n",
    "        question = question[0].lower() + question[1:]\n",
    "        q3 = f\"\"\"{q1}{q_prefix}{question}\\n{placeholder}\"\"\"\n",
    "        q2 = template.format(sys=sys2, q=q2)\n",
    "        q3 = template.format(sys=sys3, q=q3)\n",
    "        ocr_data.append({\n",
    "            \"rows\": rows,\n",
    "            \"cols\": cols,\n",
    "            \"img\": fetch_image(img),\n",
    "            \"q2\": q2,\n",
    "            \"q3\": q3,\n",
    "        })\n",
    "\n",
    "\n",
    "def postprocess():\n",
    "    submission = []\n",
    "    for i in range(length):\n",
    "        image_path = raw_data[i][\"image_path\"]\n",
    "        rows = ocr_data[i][\"rows\"]\n",
    "        cols = ocr_data[i][\"cols\"]\n",
    "        category = sub_list[final_ans2[i]]\n",
    "        answer = final_ans3[i]\n",
    "        submission.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"category\": category,\n",
    "            \"cols\": cols,\n",
    "            \"rows\": rows,\n",
    "            \"answer\": answer,\n",
    "        })\n",
    "    if len(submission) != 5360:\n",
    "        with open('error.json', 'w') as f:\n",
    "            json.dump(submission, f)\n",
    "        raise Exception(f\"Submission length is {len(submission)}\")\n",
    "    with open('submission.json', 'w') as f:\n",
    "        json.dump(submission, f)\n",
    "\n",
    "\n",
    "ocr_process = multiprocessing.Process(target=ocr)\n",
    "ocr_process.start()\n",
    "process()\n",
    "postprocess()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
